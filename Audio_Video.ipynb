{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6334c4b-dd10-40ad-9ecb-ba18b9edfd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ DEEPFAKE DETECTOR - FIXED VERSION\n",
      "üñ•Ô∏è Using device: cuda\n",
      "\n",
      "üìÇ LOADING DATASET...\n",
      "‚úÖ Successfully matched: 1000/1000 files\n",
      "üìä Label distribution: Real=252 (25.2%), Fake=748 (74.8%)\n",
      "‚úÖ Successfully matched: 1000/1000 files\n",
      "üìä Label distribution: Real=273 (27.3%), Fake=727 (72.7%)\n",
      "‚úÖ Successfully matched: 1000/1000 files\n",
      "üìä Label distribution: Real=258 (25.8%), Fake=742 (74.2%)\n",
      "Error initializing model on cuda: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 718\u001b[39m\n\u001b[32m    715\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 687\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    684\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå No valid data loaded. Please check your dataset path and metadata.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m detector = \u001b[43mDeepfakeDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m detector.feature_extractor.fit_scalers(video_audio_pairs)\n\u001b[32m    689\u001b[39m real_count = labels.count(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 522\u001b[39m, in \u001b[36mDeepfakeDetector.__init__\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = MultiModalGNN()\n\u001b[32m    521\u001b[39m     \u001b[38;5;66;03m# Move to device with validation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel successfully moved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1148\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1149\u001b[39m                     non_blocking, memory_format=convert_to_format)\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.to(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    806\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    807\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    812\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    813\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    823\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.to(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1149\u001b[39m                 non_blocking, memory_format=convert_to_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import librosa\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AudioVisualFeatureExtractor:\n",
    "    \"\"\"Extract features from audio and visual modalities - FIXED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.scaler_video = StandardScaler()\n",
    "        self.scaler_audio = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def extract_video_features(self, video_path, max_frames=30):\n",
    "        \"\"\"Extract facial and optical flow features from video - FIXED\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Warning: Could not open video {video_path}\")\n",
    "            return np.zeros((max_frames, 1024))\n",
    "        \n",
    "        facial_features = []\n",
    "        flow_features = []\n",
    "        prev_frame = None\n",
    "        frame_count = 0\n",
    "        \n",
    "        while frame_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            faces = self.face_cascade.detectMultiScale(frame_gray, 1.1, 4)\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = faces[0]\n",
    "                face_roi = frame_gray[y:y+h, x:x+w]\n",
    "                if face_roi.size > 0:\n",
    "                    face_resized = cv2.resize(face_roi, (32, 32))\n",
    "                    face_features = face_resized.flatten()\n",
    "                    if len(face_features) < 512:\n",
    "                        face_features = np.pad(face_features, (0, 512 - len(face_features)), 'constant')\n",
    "                    else:\n",
    "                        face_features = face_features[:512]\n",
    "                    facial_features.append(face_features)\n",
    "                else:\n",
    "                    facial_features.append(np.zeros(512))\n",
    "            else:\n",
    "                facial_features.append(np.zeros(512))\n",
    "            \n",
    "            if prev_frame is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_frame, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow_flat = flow.flatten()\n",
    "                if len(flow_flat) >= 512:\n",
    "                    flow_features.append(flow_flat[:512])\n",
    "                else:\n",
    "                    padded_flow = np.pad(flow_flat, (0, 512 - len(flow_flat)), 'constant')\n",
    "                    flow_features.append(padded_flow)\n",
    "            else:\n",
    "                flow_features.append(np.zeros(512))\n",
    "                \n",
    "            prev_frame = frame_gray\n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(facial_features) == 0:\n",
    "            facial_features = [np.zeros(512)]\n",
    "        if len(flow_features) == 0:\n",
    "            flow_features = [np.zeros(512)]\n",
    "            \n",
    "        facial_features = np.array(facial_features)\n",
    "        flow_features = np.array(flow_features)\n",
    "        \n",
    "        if len(facial_features) < max_frames:\n",
    "            pad_shape = ((0, max_frames - len(facial_features)), (0, 0))\n",
    "            facial_features = np.pad(facial_features, pad_shape, 'constant')\n",
    "            flow_features = np.pad(flow_features, pad_shape, 'constant')\n",
    "        else:\n",
    "            facial_features = facial_features[:max_frames]\n",
    "            flow_features = flow_features[:max_frames]\n",
    "        \n",
    "        video_features = np.concatenate([facial_features, flow_features], axis=1)\n",
    "        return video_features\n",
    "    \n",
    "    def extract_audio_features(self, audio_path, max_length=5):\n",
    "        \"\"\"Extract audio features - FIXED with debugging\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                print(f\"File not found: {audio_path}\")\n",
    "                return np.zeros(28)\n",
    "            \n",
    "            if audio_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                temp_audio = os.path.join(os.environ.get('TEMP', os.getcwd()), \"temp_audio.wav\")\n",
    "                if os.name == 'nt':\n",
    "                    null_device = 'NUL'\n",
    "                else:\n",
    "                    null_device = '/dev/null'\n",
    "                cmd = f'ffmpeg -i \"{audio_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{temp_audio}\" -y'\n",
    "                result = os.system(f'{cmd} > {null_device} 2>&1')\n",
    "                if result != 0:\n",
    "                    print(f\"ffmpeg failed with exit code {result} for {audio_path}. Command: {cmd}\")\n",
    "                    return np.zeros(28)\n",
    "                if not os.path.exists(temp_audio):\n",
    "                    print(f\"Temp file {temp_audio} not created for {audio_path}\")\n",
    "                    return np.zeros(28)\n",
    "                y, sr = librosa.load(temp_audio, sr=16000, duration=max_length)\n",
    "                os.remove(temp_audio)\n",
    "            else:\n",
    "                y, sr = librosa.load(audio_path, sr=16000, duration=max_length)\n",
    "            \n",
    "            if len(y) < sr * 0.5:\n",
    "                y = np.pad(y, (0, int(sr * 0.5) - len(y)), 'constant')\n",
    "            \n",
    "            if random.random() < 0.3:\n",
    "                noise = np.random.normal(0, 0.005, len(y))\n",
    "                y = y + noise\n",
    "            \n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            \n",
    "            features = np.hstack([\n",
    "                np.mean(mfccs, axis=1),\n",
    "                np.mean(spectral_centroids),\n",
    "                np.mean(spectral_rolloff),\n",
    "                np.mean(chroma, axis=1)\n",
    "            ])\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio {audio_path}: {e}\")\n",
    "            return np.zeros(28)\n",
    "    \n",
    "    def fit_scalers(self, video_audio_pairs):\n",
    "        \"\"\"Fit scalers on entire dataset - FIXED\"\"\"\n",
    "        all_video_features = []\n",
    "        all_audio_features = []\n",
    "        \n",
    "        for i, (video_path, audio_path) in enumerate(video_audio_pairs[:100]):\n",
    "            try:\n",
    "                video_feat = self.extract_video_features(video_path)\n",
    "                audio_feat = self.extract_audio_features(audio_path)\n",
    "                all_video_features.append(video_feat.reshape(-1, video_feat.shape[-1]))\n",
    "                all_audio_features.append(audio_feat.reshape(1, -1))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if all_video_features and all_audio_features:\n",
    "            all_video_features = np.vstack(all_video_features)\n",
    "            all_audio_features = np.vstack(all_audio_features)\n",
    "            self.scaler_video.fit(all_video_features)\n",
    "            self.scaler_audio.fit(all_audio_features)\n",
    "            self.is_fitted = True\n",
    "            print(\"‚úÖ Scalers fitted successfully\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not fit scalers - using identity scaling\")\n",
    "            self.is_fitted = False\n",
    "\n",
    "class GraphConstructor:\n",
    "    \"\"\"Construct graphs from audio-visual features - FIXED with enhanced validation\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold=0.5):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def create_graph(self, video_features, audio_features):\n",
    "        all_features = []\n",
    "        node_types = []\n",
    "        \n",
    "        # Validate and debug video features\n",
    "        if video_features.size == 0 or not np.any(video_features):\n",
    "            print(\"Warning: video_features is empty or all zeros\")\n",
    "            video_features = np.zeros((1, 1024))\n",
    "        for i, frame_feat in enumerate(video_features):\n",
    "            if frame_feat.shape[0] != 1024:\n",
    "                print(f\"Warning: Invalid frame feature shape at index {i}: {frame_feat.shape}\")\n",
    "                frame_feat = np.pad(frame_feat, (0, 1024 - frame_feat.shape[0]), 'constant')\n",
    "            all_features.append(frame_feat)\n",
    "            node_types.append(0)\n",
    "        \n",
    "        # Validate and debug audio features\n",
    "        if len(audio_features) > 0 and np.any(audio_features):\n",
    "            if audio_features.shape[0] != 28:\n",
    "                print(f\"Warning: Invalid audio feature shape: {audio_features.shape}\")\n",
    "                audio_features = np.pad(audio_features, (0, 28 - audio_features.shape[0]), 'constant')\n",
    "            audio_expanded = np.tile(audio_features, (1024 // len(audio_features)) + 1)[:1024]\n",
    "            all_features.append(audio_expanded)\n",
    "            node_types.append(1)\n",
    "        \n",
    "        if len(all_features) < 2:\n",
    "            print(\"Warning: Insufficient features, using dummy data\")\n",
    "            all_features = [np.random.randn(1024) * 0.01, np.random.randn(1024) * 0.01]\n",
    "            node_types = [0, 1]\n",
    "        \n",
    "        all_features = np.array(all_features)\n",
    "        if all_features.shape[1] != 1024:\n",
    "            print(f\"Warning: all_features has invalid feature dimension: {all_features.shape}\")\n",
    "            all_features = np.pad(all_features, ((0, 0), (0, 1024 - all_features.shape[1])), 'constant')\n",
    "        print(f\"all_features shape: {all_features.shape}\")\n",
    "        n_nodes = len(all_features)\n",
    "        \n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i + 1, n_nodes):\n",
    "                similarity = self.compute_similarity(all_features[i], all_features[j])\n",
    "                if similarity > self.similarity_threshold or node_types[i] != node_types[j]:\n",
    "                    edge_index.extend([[i, j], [j, i]])\n",
    "                    edge_attr.extend([similarity, similarity])\n",
    "        \n",
    "        if len(edge_index) == 0:\n",
    "            print(\"Warning: No edges created, adding default edges\")\n",
    "            for i in range(1, n_nodes):\n",
    "                edge_index.extend([[0, i], [i, 0]])\n",
    "                edge_attr.extend([0.3, 0.3])\n",
    "        \n",
    "        # Validate edge_index\n",
    "        edge_index = np.array(edge_index)\n",
    "        if edge_index.size > 0 and (edge_index.max() >= n_nodes or edge_index.min() < 0):\n",
    "            print(f\"Warning: Invalid edge_index detected. Max: {edge_index.max()}, Nodes: {n_nodes}\")\n",
    "            edge_index = edge_index[edge_index < n_nodes]\n",
    "        \n",
    "        x = torch.FloatTensor(all_features)\n",
    "        edge_index = torch.LongTensor(edge_index).t().contiguous() if edge_index.size > 0 else torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.FloatTensor(edge_attr) if edge_attr else torch.zeros(0)\n",
    "        node_types = torch.LongTensor(node_types)\n",
    "        batch = torch.zeros(n_nodes, dtype=torch.long)\n",
    "        \n",
    "        print(f\"edge_index shape: {edge_index.shape}, x shape: {x.shape}, node_types shape: {node_types.shape}\")\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, node_types=node_types, batch=batch)\n",
    "\n",
    "    def compute_similarity(self, feat1, feat2):\n",
    "        norm1 = np.linalg.norm(feat1)\n",
    "        norm2 = np.linalg.norm(feat2)\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        return np.dot(feat1, feat2) / (norm1 * norm2)\n",
    "\n",
    "class MultiModalGNN(nn.Module):\n",
    "    \"\"\"Graph Neural Network for multimodal deepfake detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=2, dropout=0.4):\n",
    "        super(MultiModalGNN, self).__init__()\n",
    "        self.node_type_embedding = nn.Embedding(2, 64)\n",
    "        self.video_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.audio_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gat_video = GATConv(hidden_dim + 64, hidden_dim, heads=4, dropout=dropout, concat=False)\n",
    "        self.gat_audio = GATConv(hidden_dim + 64, hidden_dim, heads=4, dropout=dropout, concat=False)\n",
    "        self.gcn = GCNConv(hidden_dim, hidden_dim // 2)\n",
    "        self.proj = nn.Linear(hidden_dim // 2, hidden_dim // 2)\n",
    "        self.cross_attention = nn.MultiheadAttention(hidden_dim // 2, num_heads=4, dropout=dropout, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Validate initial parameters\n",
    "        for name, param in self.named_parameters():\n",
    "            if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "                print(f\"Warning: Invalid parameter detected in {name}: {param}\")\n",
    "    \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            x, edge_index, batch, node_types = data.x, data.edge_index, data.batch, data.node_types\n",
    "            print(f\"Forward pass - x shape: {x.shape}, edge_index shape: {edge_index.shape}, node_types shape: {node_types.shape}\")\n",
    "            \n",
    "            # Validate node_types\n",
    "            if not torch.all((node_types >= 0) & (node_types < 2)):\n",
    "                print(f\"Warning: Invalid node_types values: {node_types}\")\n",
    "                node_types = torch.clamp(node_types, 0, 1)\n",
    "            \n",
    "            type_emb = self.node_type_embedding(node_types)\n",
    "            \n",
    "            video_mask = node_types == 0\n",
    "            audio_mask = node_types == 1\n",
    "            \n",
    "            if video_mask.sum() > x.size(0) or audio_mask.sum() > x.size(0):\n",
    "                print(f\"Warning: Mask size exceeds x size - video_mask: {video_mask.sum()}, audio_mask: {audio_mask.sum()}, x size: {x.size(0)}\")\n",
    "                video_mask = video_mask[:x.size(0)]\n",
    "                audio_mask = audio_mask[:x.size(0)]\n",
    "            \n",
    "            x_projected = torch.zeros(x.size(0), self.video_proj.out_features, device=x.device)\n",
    "            \n",
    "            if video_mask.any():\n",
    "                x_projected[video_mask] = F.relu(self.video_proj(x[video_mask]))\n",
    "            if audio_mask.any():\n",
    "                x_projected[audio_mask] = F.relu(self.audio_proj(x[audio_mask]))\n",
    "            \n",
    "            x_with_type = torch.cat([x_projected, type_emb], dim=1)\n",
    "            \n",
    "            x_gat = torch.zeros(x.size(0), self.gat_video.out_channels, device=x.device)\n",
    "            \n",
    "            if video_mask.any():\n",
    "                x_gat[video_mask] = F.relu(self.gat_video(x_with_type[video_mask], edge_index))\n",
    "            if audio_mask.any():\n",
    "                x_gat[audio_mask] = F.relu(self.gat_audio(x_with_type[audio_mask], edge_index))\n",
    "            \n",
    "            x = F.relu(self.gcn(x_gat, edge_index))\n",
    "            x = self.proj(x)\n",
    "            \n",
    "            num_graphs = batch.max().item() + 1\n",
    "            total_nodes = x.size(0)\n",
    "            expected_feature_dim = 128\n",
    "            num_nodes_per_graph = (total_nodes + num_graphs - 1) // num_graphs\n",
    "            \n",
    "            padding_size = (num_graphs * num_nodes_per_graph) - total_nodes\n",
    "            if padding_size > 0:\n",
    "                padding = torch.zeros(padding_size, expected_feature_dim, device=x.device)\n",
    "                x = torch.cat([x, padding], dim=0)\n",
    "            \n",
    "            total_nodes = x.size(0)\n",
    "            num_nodes = total_nodes // num_graphs\n",
    "            \n",
    "            x_reshaped = x.view(num_graphs, num_nodes, expected_feature_dim).transpose(0, 1)\n",
    "            \n",
    "            attn_output, _ = self.cross_attention(x_reshaped, x_reshaped, x_reshaped)\n",
    "            \n",
    "            x = attn_output.transpose(0, 1).reshape(-1, expected_feature_dim)\n",
    "            \n",
    "            if padding_size > 0:\n",
    "                batch_padded = torch.cat([batch, torch.full((padding_size,), -1, device=batch.device)])\n",
    "                mask = batch_padded >= 0\n",
    "                x_masked = x[mask]\n",
    "                batch_masked = batch_padded[mask]\n",
    "                graph_repr = torch.cat([global_mean_pool(x_masked, batch_masked), global_max_pool(x_masked, batch_masked)], dim=1)\n",
    "            else:\n",
    "                graph_repr = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch)], dim=1)\n",
    "            \n",
    "            return self.classifier(graph_repr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            raise\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for imbalanced datasets\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "def extract_label_from_entry(entry, video_path):\n",
    "    \"\"\"Extract label from metadata entry with multiple strategies\"\"\"\n",
    "    if 'n_fakes' in entry:\n",
    "        n_fakes = entry['n_fakes']\n",
    "        if isinstance(n_fakes, int):\n",
    "            return 1 if n_fakes > 0 else 0\n",
    "    \n",
    "    if 'fake_periods' in entry:\n",
    "        fake_periods = entry['fake_periods']\n",
    "        if isinstance(fake_periods, list):\n",
    "            return 1 if len(fake_periods) > 0 else 0\n",
    "    \n",
    "    if 'label' in entry:\n",
    "        label = entry['label']\n",
    "        if isinstance(label, str):\n",
    "            return 1 if label.lower() in ['fake', 'deepfake', '1'] else 0\n",
    "        elif isinstance(label, int):\n",
    "            return label\n",
    "    \n",
    "    if 'is_fake' in entry:\n",
    "        return 1 if entry['is_fake'] else 0\n",
    "    \n",
    "    if 'original' in entry:\n",
    "        return 0 if entry['original'] else 1\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è No valid label field found for {video_path}\")\n",
    "    return None\n",
    "\n",
    "def create_filename_mapping(metadata):\n",
    "    \"\"\"Create a mapping of all possible filename variations to metadata entries\"\"\"\n",
    "    filename_map = {}\n",
    "    \n",
    "    for entry in metadata:\n",
    "        if not isinstance(entry, dict) or 'file' not in entry:\n",
    "            continue\n",
    "            \n",
    "        metadata_file = entry['file']\n",
    "        variations = [\n",
    "            metadata_file,\n",
    "            os.path.basename(metadata_file),\n",
    "            os.path.splitext(os.path.basename(metadata_file))[0],\n",
    "            metadata_file.replace('\\\\', '/'),\n",
    "            metadata_file.replace('/', '\\\\'),\n",
    "        ]\n",
    "        \n",
    "        for variation in variations:\n",
    "            if variation not in filename_map:\n",
    "                filename_map[variation] = []\n",
    "            filename_map[variation].append(entry)\n",
    "    \n",
    "    return filename_map\n",
    "\n",
    "def find_label_with_mapping(video_path, filename_map, dataset_path):\n",
    "    \"\"\"Find label using the filename mapping with multiple strategies\"\"\"\n",
    "    video_rel_path = os.path.relpath(video_path, dataset_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    video_name_no_ext = os.path.splitext(video_name)[0]\n",
    "    \n",
    "    search_keys = [\n",
    "        video_rel_path,\n",
    "        video_rel_path.replace('\\\\', '/'),\n",
    "        video_rel_path.replace('/', '\\\\'),\n",
    "        video_name,\n",
    "        video_name_no_ext,\n",
    "    ]\n",
    "    \n",
    "    for key in search_keys:\n",
    "        if key in filename_map:\n",
    "            entry = filename_map[key][0]\n",
    "            label = extract_label_from_entry(entry, video_path)\n",
    "            if label is not None:\n",
    "                return label\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_lavdf_dataset_improved(dataset_path, use_subset='train', max_clips=1000):\n",
    "    \"\"\"Improved dataset loading with better debugging and matching\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(\"‚ùå metadata.json not found!\")\n",
    "        return [], []\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    filename_map = create_filename_mapping(metadata)\n",
    "    \n",
    "    subset_folder = os.path.join(dataset_path, use_subset)\n",
    "    if not os.path.exists(subset_folder):\n",
    "        print(f\"‚ùå Subset folder '{use_subset}' not found\")\n",
    "        return [], []\n",
    "    \n",
    "    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "    all_videos = []\n",
    "    for ext in video_extensions:\n",
    "        all_videos.extend(glob.glob(os.path.join(subset_folder, '**', ext), recursive=True))\n",
    "    \n",
    "    if len(all_videos) > max_clips:\n",
    "        random.shuffle(all_videos)\n",
    "        all_videos = all_videos[:max_clips]\n",
    "    \n",
    "    video_audio_pairs = []\n",
    "    labels = []\n",
    "    matched_count = 0\n",
    "    unmatched_files = []\n",
    "    \n",
    "    for video_path in all_videos:\n",
    "        label = find_label_with_mapping(video_path, filename_map, dataset_path)\n",
    "        \n",
    "        if label is not None:\n",
    "            video_audio_pairs.append((video_path, video_path))\n",
    "            labels.append(label)\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            unmatched_files.append(video_path)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully matched: {matched_count}/{len(all_videos)} files\")\n",
    "    if labels:\n",
    "        real_count = labels.count(0)\n",
    "        fake_count = labels.count(1)\n",
    "        total = real_count + fake_count\n",
    "        print(f\"üìä Label distribution: Real={real_count} ({real_count/total*100:.1f}%), Fake={fake_count} ({fake_count/total*100:.1f}%)\")\n",
    "    \n",
    "    return video_audio_pairs, labels\n",
    "\n",
    "def load_all_subsets(dataset_path, max_clips=3000):\n",
    "    \"\"\"Load all subsets and combine - targeting 3000 total with fair distribution\"\"\"\n",
    "    all_pairs = []\n",
    "    all_labels = []\n",
    "    clips_per_subset = max_clips // 3\n",
    "    \n",
    "    for subset in ['train', 'test', 'dev']:\n",
    "        subset_path = os.path.join(dataset_path, subset)\n",
    "        if os.path.exists(subset_path):\n",
    "            pairs, labels = load_lavdf_dataset_improved(dataset_path, use_subset=subset, max_clips=clips_per_subset)\n",
    "            if pairs and labels:\n",
    "                all_pairs.extend(pairs)\n",
    "                all_labels.extend(labels)\n",
    "    \n",
    "    if len(all_pairs) > max_clips:\n",
    "        all_pairs = all_pairs[:max_clips]\n",
    "        all_labels = all_labels[:max_clips]\n",
    "    \n",
    "    return all_pairs, all_labels\n",
    "\n",
    "class DeepfakeDetector:\n",
    "    \"\"\"Main detector class\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.feature_extractor = AudioVisualFeatureExtractor(device)\n",
    "        self.graph_constructor = GraphConstructor(similarity_threshold=0.7)\n",
    "        try:\n",
    "            self.model = MultiModalGNN()\n",
    "            # Move to device with validation\n",
    "            self.model = self.model.to(device)\n",
    "            print(f\"Model successfully moved to {device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model on {device}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_data(self, video_audio_pairs, labels, feature_dir=None):\n",
    "        \"\"\"Prepare graph data from video-audio pairs or precomputed features\"\"\"\n",
    "        graphs = []\n",
    "        for i, ((video_path, audio_path), label) in enumerate(zip(video_audio_pairs, labels)):\n",
    "            try:\n",
    "                if feature_dir and os.path.exists(os.path.join(feature_dir, f\"video_{i}.npy\")):\n",
    "                    video_features, audio_features = self.feature_extractor.load_features(feature_dir, i)\n",
    "                else:\n",
    "                    video_features = self.feature_extractor.extract_video_features(video_path)\n",
    "                    audio_features = self.feature_extractor.extract_audio_features(audio_path)\n",
    "                \n",
    "                graph = self.graph_constructor.create_graph(video_features, audio_features)\n",
    "                graph.y = torch.tensor([label], dtype=torch.long)\n",
    "                graphs.append(graph)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "                continue\n",
    "        return graphs\n",
    "    \n",
    "    def train(self, train_graphs, val_graphs, epochs=50, batch_size=4, accum_steps=2):\n",
    "        \"\"\"Train the model with gradient accumulation\"\"\"\n",
    "        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "        criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for i, batch in enumerate(train_loader):\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(batch)\n",
    "                    loss = criterion(outputs, batch.y) / accum_steps\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % accum_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    train_loss += loss.item() * accum_steps\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    train_total += batch.y.size(0)\n",
    "                    train_correct += (predicted == batch.y).sum().item()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training batch {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    try:\n",
    "                        batch = batch.to(self.device)\n",
    "                        outputs = self.model(batch)\n",
    "                        loss = criterion(outputs, batch.y)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        val_total += batch.y.size(0)\n",
    "                        val_correct += (predicted == batch.y).sum().item()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in validation batch: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            print(f'Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "                print(f'üíæ New best model saved with accuracy: {val_acc:.2f}%')\n",
    "            if val_acc >= 80.0:\n",
    "                print(f\"üéØ Target accuracy reached! Final accuracy: {val_acc:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def evaluate(self, test_graphs):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(test_graphs, batch_size=4, shuffle=False)\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    outputs = self.model(batch)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(batch.y.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "        \n",
    "        print(f\"üìä Test Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "def debug_metadata_structure(dataset_path):\n",
    "    \"\"\"Debug the metadata structure\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(\"‚ùå metadata.json not found!\")\n",
    "        return None\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    if metadata:\n",
    "        sample_keys = set()\n",
    "        for entry in metadata[:10]:\n",
    "            if isinstance(entry, dict):\n",
    "                sample_keys.update(entry.keys())\n",
    "        print(f\"üìä Metadata entries: {len(metadata)}, Keys: {sample_keys}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ DEEPFAKE DETECTOR - FIXED VERSION\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "    \n",
    "    dataset_path = r\"C:\\archive\\LAV-DF\"\n",
    "    feature_dir = r\"C:\\Users\\ARNAV\\features\"  \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Dataset path does not exist: {dataset_path}\")\n",
    "        return\n",
    "    if not os.path.exists(feature_dir):\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nüìÇ LOADING DATASET...\")\n",
    "    video_audio_pairs, labels = load_all_subsets(dataset_path, max_clips=3000)\n",
    "    \n",
    "    if len(video_audio_pairs) == 0:\n",
    "        print(\"‚ùå No valid data loaded. Please check your dataset path and metadata.\")\n",
    "        return\n",
    "        \n",
    "    detector = DeepfakeDetector(device=device)\n",
    "    detector.feature_extractor.fit_scalers(video_audio_pairs)\n",
    "    real_count = labels.count(0)\n",
    "    fake_count = labels.count(1)\n",
    "    print(f\"\\nüìä FINAL DATASET STATISTICS: Total samples: {len(video_audio_pairs)}, Real: {real_count} ({real_count/(real_count+fake_count)*100:.1f}%), Fake: {fake_count} ({fake_count/(real_count+fake_count)*100:.1f}%)\")\n",
    "    \n",
    "    if real_count == 0 or fake_count == 0:\n",
    "        print(\"‚ùå Dataset is severely imbalanced. Check metadata parsing.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è PRECOMPUTING FEATURES...\")\n",
    "    if not os.path.exists(feature_dir):\n",
    "        detector.feature_extractor.save_features(video_audio_pairs, feature_dir)\n",
    "    \n",
    "    graphs = detector.prepare_data(video_audio_pairs, labels, feature_dir=feature_dir)\n",
    "    if len(graphs) < 10:\n",
    "        print(f\"‚ö†Ô∏è Only {len(graphs)} graphs created\")\n",
    "        return\n",
    "    \n",
    "    train_graphs, temp_graphs = train_test_split(graphs, test_size=0.3, random_state=42)\n",
    "    val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42)\n",
    "    print(f\"Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}\")\n",
    "    \n",
    "    best_acc = detector.train(train_graphs, val_graphs, epochs=50, batch_size=4, accum_steps=2)\n",
    "    print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    if test_graphs:\n",
    "        accuracy, precision, recall, f1 = detector.evaluate(test_graphs)\n",
    "        print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fa52e-762c-4762-bff2-f1c34acea49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b3cb2-dbd6-4a86-9394-f520f8d525b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
