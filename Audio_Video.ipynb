{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6334c4b-dd10-40ad-9ecb-ba18b9edfd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import librosa\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Enable CUDA debugging (disabled for CPU)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Print PyTorch and CUDA versions for debugging\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"Custom collate function to debug batch construction.\"\"\"\n",
    "    print(\"Debug: Entering custom_collate function\")\n",
    "    print(f\"Debug: Collating batch with {len(batch)} graphs\")\n",
    "    for i, data in enumerate(batch):\n",
    "        print(f\"Debug: Graph {i} - nodes={data.x.size(0)}, edge_index_max={data.edge_index.max().item() if data.edge_index.numel() > 0 else -1}, node_types={data.node_types.unique()}, label={data.y.item()}\")\n",
    "    batch = Batch.from_data_list(batch)\n",
    "    print(f\"Debug: Collated batch - nodes={batch.x.size(0)}, edge_index_max={batch.edge_index.max().item() if batch.edge_index.numel() > 0 else -1}, node_types={batch.node_types.unique()}, batch_size={batch.batch.max().item() + 1 if batch.batch.numel() > 0 else 1}\")\n",
    "    return batch\n",
    "    \n",
    "def validate_edge_index(data: Data) -> Data:\n",
    "    \"\"\"Clamp `edge_index` so every entry is < num_nodes and keep `edge_attr` in sync.\"\"\"\n",
    "    if data.edge_index.numel() == 0:\n",
    "        print(\"Debug: Empty edge_index, returning unchanged\")\n",
    "        return data\n",
    "\n",
    "    max_nodes = data.x.size(0)\n",
    "    print(f\"Debug: Validating edge_index, max_nodes={max_nodes}, edge_index_max={data.edge_index.max().item() if data.edge_index.numel() > 0 else -1}\")\n",
    "    mask = (data.edge_index[0] < max_nodes) & (data.edge_index[1] < max_nodes)\n",
    "    if not mask.all():\n",
    "        print(f\"Debug: Filtering {(~mask).sum()} invalid edges with indices >= {max_nodes}\")\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None and data.edge_attr.size(0) == mask.size(0):\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    return data\n",
    "\n",
    "class AudioVisualFeatureExtractor:\n",
    "    \"\"\"Extract features from audio and visual modalities with normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.scaler_video = StandardScaler()\n",
    "        self.scaler_audio = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def extract_video_features(self, video_path, max_frames=30):\n",
    "        \"\"\"Extract facial and optical flow features from video.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Warning: Could not open video {video_path}\")\n",
    "            return np.zeros((max_frames, 1024))\n",
    "        \n",
    "        facial_features = []\n",
    "        flow_features = []\n",
    "        prev_frame = None\n",
    "        frame_count = 0\n",
    "        \n",
    "        while frame_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            faces = self.face_cascade.detectMultiScale(frame_gray, 1.1, 4)\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = faces[0]\n",
    "                face_roi = frame_gray[y:y+h, x:x+w]\n",
    "                if face_roi.size > 0:\n",
    "                    face_resized = cv2.resize(face_roi, (32, 32))\n",
    "                    face_features = face_resized.flatten()\n",
    "                    if len(face_features) < 512:\n",
    "                        face_features = np.pad(face_features, (0, 512 - len(face_features)), 'constant')\n",
    "                    else:\n",
    "                        face_features = face_features[:512]\n",
    "                    facial_features.append(face_features)\n",
    "                else:\n",
    "                    facial_features.append(np.zeros(512))\n",
    "            else:\n",
    "                facial_features.append(np.zeros(512))\n",
    "            \n",
    "            if prev_frame is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_frame, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow_flat = flow.flatten()\n",
    "                if len(flow_flat) >= 512:\n",
    "                    flow_features.append(flow_flat[:512])\n",
    "                else:\n",
    "                    padded_flow = np.pad(flow_flat, (0, 512 - len(flow_flat)), 'constant')\n",
    "                    flow_features.append(padded_flow)\n",
    "            else:\n",
    "                flow_features.append(np.zeros(512))\n",
    "                \n",
    "            prev_frame = frame_gray\n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(facial_features) == 0:\n",
    "            facial_features = [np.zeros(512)]\n",
    "        if len(flow_features) == 0:\n",
    "            flow_features = [np.zeros(512)]\n",
    "            \n",
    "        facial_features = np.array(facial_features)\n",
    "        flow_features = np.array(flow_features)\n",
    "        \n",
    "        if len(facial_features) < max_frames:\n",
    "            pad_shape = ((0, max_frames - len(facial_features)), (0, 0))\n",
    "            facial_features = np.pad(facial_features, pad_shape, 'constant')\n",
    "            flow_features = np.pad(flow_features, pad_shape, 'constant')\n",
    "        else:\n",
    "            facial_features = facial_features[:max_frames]\n",
    "            flow_features = flow_features[:max_frames]\n",
    "        \n",
    "        video_features = np.concatenate([facial_features, flow_features], axis=1)\n",
    "        if self.is_fitted:\n",
    "            video_features = self.scaler_video.transform(video_features)\n",
    "        return video_features\n",
    "    \n",
    "    def extract_audio_features(self, audio_path, max_length=5):\n",
    "        \"\"\"Extract audio features with robust error handling and normalization.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                print(f\"File not found: {audio_path}\")\n",
    "                return np.zeros(35)\n",
    "\n",
    "            import subprocess\n",
    "            ffprobe_cmd = f'ffprobe -i \"{audio_path}\" -show_streams -select_streams a -loglevel error'\n",
    "            result = subprocess.run(ffprobe_cmd, capture_output=True, text=True, shell=True)\n",
    "            if not result.stdout:\n",
    "                print(f\"Warning: No audio track found in {audio_path}\")\n",
    "                return np.zeros(35)\n",
    "\n",
    "            if audio_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                temp_audio = os.path.join(os.environ.get('TEMP', os.getcwd()), \"temp_audio.wav\")\n",
    "                if os.name == 'nt':\n",
    "                    null_device = 'NUL'\n",
    "                else:\n",
    "                    null_device = '/dev/null'\n",
    "                cmd = f'ffmpeg -i \"{audio_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{temp_audio}\" -y'\n",
    "                result = os.system(f'{cmd} > {null_device} 2>&1')\n",
    "                if result != 0:\n",
    "                    print(f\"ffmpeg failed with exit code {result} for {audio_path}. Command: {cmd}\")\n",
    "                    return np.zeros(35)\n",
    "                if not os.path.exists(temp_audio):\n",
    "                    print(f\"Temp file {temp_audio} not created for {audio_path}\")\n",
    "                    return np.zeros(35)\n",
    "                y, sr = librosa.load(temp_audio, sr=16000, duration=max_length)\n",
    "                os.remove(temp_audio)\n",
    "            else:\n",
    "                y, sr = librosa.load(audio_path, sr=16000, duration=max_length)\n",
    "\n",
    "            if len(y) == 0 or np.all(y == 0):\n",
    "                print(f\"Warning: Empty or invalid audio data for {audio_path}\")\n",
    "                return np.zeros(35)\n",
    "\n",
    "            if len(y) < sr * 0.5:\n",
    "                y = np.pad(y, (0, int(sr * 0.5) - len(y)), 'constant')\n",
    "\n",
    "            if random.random() < 0.3:\n",
    "                noise = np.random.normal(0, 0.005, len(y))\n",
    "                y = y + noise\n",
    "\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            zero_crossings = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "            if mfccs.shape[1] == 0 or spectral_centroids.shape[1] == 0:\n",
    "                print(f\"Warning: Invalid feature dimensions for {audio_path}\")\n",
    "                return np.zeros(35)\n",
    "\n",
    "            features = np.hstack([\n",
    "                np.mean(mfccs, axis=1),\n",
    "                np.mean(spectral_centroids),\n",
    "                np.mean(spectral_rolloff),\n",
    "                np.mean(chroma, axis=1),\n",
    "                np.mean(zero_crossings)\n",
    "            ])\n",
    "            expected_size = 20 + 1 + 1 + 12 + 1\n",
    "            if features.shape[0] != expected_size:\n",
    "                print(f\"Warning: Audio feature shape {features.shape[0]} does not match expected {expected_size}\")\n",
    "                features = np.pad(features, (0, expected_size - features.shape[0]), 'constant') if features.shape[0] < expected_size else features[:expected_size]\n",
    "            if self.is_fitted:\n",
    "                features = self.scaler_audio.transform(features.reshape(1, -1)).flatten()\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio {audio_path}: {e}\")\n",
    "            return np.zeros(35)\n",
    "    \n",
    "    def fit_scalers(self, video_audio_pairs):\n",
    "        \"\"\"Fit scalers on entire dataset.\"\"\"\n",
    "        all_video_features = []\n",
    "        all_audio_features = []\n",
    "        \n",
    "        for i, (video_path, audio_path) in enumerate(video_audio_pairs[:100]):\n",
    "            try:\n",
    "                video_feat = self.extract_video_features(video_path)\n",
    "                audio_feat = self.extract_audio_features(audio_path)\n",
    "                all_video_features.append(video_feat.reshape(-1, video_feat.shape[-1]))\n",
    "                all_audio_features.append(audio_feat.reshape(1, -1))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if all_video_features and all_audio_features:\n",
    "            all_video_features = np.vstack(all_video_features)\n",
    "            all_audio_features = np.vstack(all_audio_features)\n",
    "            self.scaler_video.fit(all_video_features)\n",
    "            self.scaler_audio.fit(all_audio_features)\n",
    "            self.is_fitted = True\n",
    "            print(\"‚úÖ Scalers fitted successfully\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not fit scalers - using identity scaling\")\n",
    "            self.is_fitted = False\n",
    "    \n",
    "    def save_features(self, video_audio_pairs, feature_dir):\n",
    "        \"\"\"Save extracted features to disk with validation.\"\"\"\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "        for i, (video_path, audio_path) in enumerate(video_audio_pairs):\n",
    "            try:\n",
    "                video_features = self.extract_video_features(video_path)\n",
    "                audio_features = self.extract_audio_features(audio_path)\n",
    "                if video_features.shape != (30, 1024):\n",
    "                    print(f\"Warning: Invalid video feature shape {video_features.shape} for {video_path}\")\n",
    "                    video_features = np.zeros((30, 1024))\n",
    "                if audio_features.shape != (35,):\n",
    "                    print(f\"Warning: Invalid audio feature shape {audio_features.shape} for {audio_path}\")\n",
    "                    audio_features = np.zeros(35)\n",
    "                video_file = os.path.join(feature_dir, f\"video_{i}.npy\")\n",
    "                audio_file = os.path.join(feature_dir, f\"audio_{i}.npy\")\n",
    "                np.save(video_file, video_features)\n",
    "                np.save(audio_file, audio_features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving features for {video_path}: {e}\")\n",
    "                continue\n",
    "        print(f\"‚úÖ Saved features to {feature_dir}\")\n",
    "\n",
    "class GraphConstructor:\n",
    "    \"\"\"Construct graphs from audio-visual features with enhanced audio connectivity.\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold=0.6):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def _finalise_edges(self, edge_index_list, edge_attr_list, n_nodes):\n",
    "        \"\"\"Convert lists to tensors and drop invalid edges.\"\"\"\n",
    "        if not edge_index_list:\n",
    "            print(\"Warning: No edges created, returning empty edge tensors\")\n",
    "            return torch.zeros((2, 0), dtype=torch.long), torch.zeros(0)\n",
    "\n",
    "        ei = np.asarray(edge_index_list, dtype=np.int64)\n",
    "        ea = np.asarray(edge_attr_list, dtype=np.float32)\n",
    "        valid = (ei[:, 0] < n_nodes) & (ei[:, 1] < n_nodes)\n",
    "        if not valid.all():\n",
    "            print(f\"Debug: Filtering out {(~valid).sum()} invalid edges with indices >= {n_nodes}\")\n",
    "        ei = ei[valid]\n",
    "        ea = ea[valid] if ea.shape[0] == valid.size else ea[:valid.sum()]\n",
    "        if ei.size == 0:\n",
    "            print(\"Warning: All edges filtered out, returning empty edge tensors\")\n",
    "            return torch.zeros((2, 0), dtype=torch.long), torch.zeros(0)\n",
    "        print(f\"Debug: Finalized {ei.shape[0]} edges for {n_nodes} nodes\")\n",
    "        return torch.from_numpy(ei).t().contiguous(), torch.from_numpy(ea)\n",
    "        \n",
    "    def create_graph(self, video_features: np.ndarray, audio_features: np.ndarray) -> Data:\n",
    "        all_features, node_types = [], []\n",
    "        for frame in video_features:\n",
    "            all_features.append(frame)\n",
    "            node_types.append(0)\n",
    "        print(f\"Debug: Added {len(all_features)} video nodes\")\n",
    "        \n",
    "        audio_added = False\n",
    "        if audio_features.size > 0 and np.any(audio_features) and audio_features.shape[0] == 35:\n",
    "            audio_expanded = np.tile(audio_features, (1024 // audio_features.size + 1))[:1024]\n",
    "            all_features.append(audio_expanded)\n",
    "            node_types.append(1)\n",
    "            audio_added = True\n",
    "            print(f\"Debug: Audio node added, total nodes = {len(all_features)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Audio features invalid or incorrect shape, using default features\")\n",
    "            all_features.append(np.zeros(1024))\n",
    "            node_types.append(1)\n",
    "            audio_added = True\n",
    "            print(f\"Debug: Default audio node added, total nodes = {len(all_features)}\")\n",
    "\n",
    "        if len(all_features) < 2:\n",
    "            print(\"Warning: Fewer than 2 nodes, adding dummy nodes\")\n",
    "            all_features.extend([np.zeros(1024), np.zeros(1024)])\n",
    "            node_types.extend([0, 1])\n",
    "\n",
    "        all_features = np.asarray(all_features, dtype=np.float32)\n",
    "        n_nodes = all_features.shape[0]\n",
    "        print(f\"Debug: Total number of nodes = {n_nodes}\")\n",
    "\n",
    "        edge_index_list, edge_attr_list = [], []\n",
    "        audio_node_idx = n_nodes - 1 if audio_added else None\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i + 1, n_nodes):\n",
    "                if node_types[i] != node_types[j]:\n",
    "                    edge_index_list.extend([[i, j], [j, i]])\n",
    "                    edge_attr_list.extend([0.5, 0.5])\n",
    "                else:\n",
    "                    sim = self._cosine(all_features[i], all_features[j])\n",
    "                    if sim > self.similarity_threshold:\n",
    "                        edge_index_list.extend([[i, j], [j, i]])\n",
    "                        edge_attr_list.extend([sim, sim])\n",
    "        if audio_added and audio_node_idx is not None:\n",
    "            audio_edges = [[i, audio_node_idx] for i in range(n_nodes-1) if node_types[i] == 0]\n",
    "            audio_edges.extend([[audio_node_idx, i] for i in range(n_nodes-1) if node_types[i] == 0])\n",
    "            edge_index_list.extend(audio_edges)\n",
    "            edge_attr_list.extend([0.5] * len(audio_edges))\n",
    "            print(f\"Debug: Added {len(audio_edges)} explicit audio-video edges\")\n",
    "\n",
    "        print(f\"Debug: Created {len(edge_index_list)} initial edges (including cross-modal)\")\n",
    "\n",
    "        edge_index, edge_attr = self._finalise_edges(edge_index_list, edge_attr_list, n_nodes)\n",
    "        if not edge_index_list:\n",
    "            print(\"Warning: No edges created, adding fallback edges\")\n",
    "            for j in range(1, min(n_nodes, 2)):\n",
    "                edge_index_list.extend([[0, j], [j, 0]])\n",
    "                edge_attr_list.extend([0.0, 0.0])\n",
    "            print(f\"Debug: Added {len(edge_index_list)} fallback edges\")\n",
    "            edge_index, edge_attr = self._finalise_edges(edge_index_list, edge_attr_list, n_nodes)\n",
    "\n",
    "        print(f\"Debug: Final edge count = {edge_index.shape[1]}, max index = {edge_index.max().item() if edge_index.numel() > 0 else -1}\")\n",
    "\n",
    "        data = Data(\n",
    "            x=torch.from_numpy(all_features).to(torch.device('cpu')),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            node_types=torch.tensor(node_types, dtype=torch.long).to(torch.device('cpu')),\n",
    "            batch=torch.zeros(n_nodes, dtype=torch.long).to(torch.device('cpu')),\n",
    "        )\n",
    "        data = validate_edge_index(data)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine(a, b):\n",
    "        na, nb = np.linalg.norm(a), np.linalg.norm(b)\n",
    "        return 0.0 if na == 0 or nb == 0 else float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "class MultiModalGNN(nn.Module):\n",
    "    \"\"\"Graph Neural Network for multimodal deepfake detection with robust audio handling.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 1024, hidden_dim: int = 256, num_classes: int = 2, dropout: float = 0.4, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.node_type_embedding = nn.Embedding(2, 64).to(self.device)\n",
    "        self.video_proj = nn.Linear(input_dim, hidden_dim).to(self.device)\n",
    "        self.audio_proj = nn.Linear(input_dim, hidden_dim).to(self.device)\n",
    "        self.gat_video = GATConv(hidden_dim + 64, hidden_dim, heads=4, dropout=dropout, concat=False).to(self.device)\n",
    "        self.gat_audio = GATConv(hidden_dim + 64, hidden_dim, heads=4, dropout=dropout, concat=False).to(self.device)\n",
    "        self.gcn = GCNConv(hidden_dim, hidden_dim // 2).to(self.device)\n",
    "        self.proj = nn.Linear(hidden_dim // 2, hidden_dim // 2).to(self.device)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim // 2, num_heads=4, dropout=dropout, batch_first=True).to(self.device)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        ).to(self.device)\n",
    "    \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        data = data.to(self.device)\n",
    "        x, ei, batch, node_types = data.x.to(self.device), data.edge_index.to(self.device), data.batch.to(self.device), data.node_types.to(self.device)\n",
    "    \n",
    "        print(f\"Debug: Forward pass - nodes={x.size(0)}, max edge_index={ei.max().item() if ei.numel() > 0 else -1}, node_types={node_types.unique()}, batch_size={batch.max().item() + 1 if batch.numel() > 0 else 1}\")\n",
    "        print(f\"Debug: Node features shape={x.shape}, node_types shape={node_types.shape}, batch shape={batch.shape}\")\n",
    "    \n",
    "        try:\n",
    "            if node_types.min() < 0 or node_types.max() > 1:\n",
    "                print(f\"Warning: node_types out of range: {node_types.unique()}\")\n",
    "            if hasattr(data, 'y') and (data.y.min() < 0 or data.y.max() > 1):\n",
    "                print(f\"Warning: labels out of range: {data.y.unique()}\")\n",
    "            if ei.numel() > 0 and ei.max() >= x.size(0):\n",
    "                print(f\"Warning: edge_index out of bounds: max {ei.max()}, nodes {x.size(0)}\")\n",
    "                mask = (ei[0] < x.size(0)) & (ei[1] < x.size(0))\n",
    "                ei = ei[:, mask]\n",
    "                if data.edge_attr is not None and data.edge_attr.size(0) == mask.size(0):\n",
    "                    data.edge_attr = data.edge_attr[mask]\n",
    "                data.edge_index = ei\n",
    "                print(f\"Debug: Clamped edge_index to {ei.shape[1]} edges, max index={ei.max().item() if ei.numel() > 0 else -1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Forward pass validation warning: {e}\")\n",
    "            return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "    \n",
    "        type_emb = self.node_type_embedding(node_types)\n",
    "        video_mask = node_types == 0\n",
    "        audio_mask = node_types == 1\n",
    "    \n",
    "        x_proj = torch.zeros(x.size(0), self.video_proj.out_features, device=self.device)\n",
    "        if video_mask.any():\n",
    "            x_proj[video_mask] = F.relu(self.video_proj(x[video_mask]))\n",
    "        if audio_mask.any():\n",
    "            x_proj[audio_mask] = F.relu(self.audio_proj(x[audio_mask]))\n",
    "    \n",
    "        x = torch.cat([x_proj, type_emb], dim=1)\n",
    "    \n",
    "        h = torch.zeros_like(x_proj, device=self.device)\n",
    "    \n",
    "        if video_mask.any():\n",
    "            video_indices = torch.where(video_mask)[0]\n",
    "            video_mask_edge = torch.isin(ei[0], video_indices) & torch.isin(ei[1], video_indices)\n",
    "            video_edge_index = ei[:, video_mask_edge]\n",
    "            if video_edge_index.numel() > 0:\n",
    "                video_node_map = torch.zeros(x.size(0), dtype=torch.long, device=self.device)\n",
    "                video_node_map[video_indices] = torch.arange(video_indices.size(0), device=self.device)\n",
    "                video_edge_index = video_node_map[video_edge_index]\n",
    "                try:\n",
    "                    print(f\"Debug: Video GAT - nodes={x[video_mask].size(0)}, edges={video_edge_index.shape[1]}, max edge_index={video_edge_index.max().item() if video_edge_index.numel() > 0 else -1}\")\n",
    "                    h[video_mask] = F.relu(self.gat_video(x[video_mask], video_edge_index))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: GATConv video failed: {e}\")\n",
    "            else:\n",
    "                print(\"Warning: No valid edges for video nodes, skipping GATConv video\")\n",
    "    \n",
    "        if audio_mask.any():\n",
    "            audio_indices = torch.where(audio_mask)[0]\n",
    "            audio_mask_edge = torch.isin(ei[0], audio_indices) | torch.isin(ei[1], audio_indices)\n",
    "            audio_edge_index = ei[:, audio_mask_edge]\n",
    "            if audio_edge_index.numel() > 0:\n",
    "                audio_node_map = torch.zeros(x.size(0), dtype=torch.long, device=self.device)\n",
    "                audio_node_map[audio_indices] = torch.arange(audio_indices.size(0), device=self.device)\n",
    "                audio_edge_index = audio_node_map[audio_edge_index]\n",
    "                try:\n",
    "                    print(f\"Debug: Audio GAT - nodes={x[audio_mask].size(0)}, edges={audio_edge_index.shape[1]}, max edge_index={audio_edge_index.max().item() if audio_edge_index.numel() > 0 else -1}\")\n",
    "                    h[audio_mask] = F.relu(self.gat_audio(x[audio_mask], audio_edge_index))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: GATConv audio failed: {e}\")\n",
    "            else:\n",
    "                print(\"Warning: No valid edges for audio nodes, adding default cross-modal edges\")\n",
    "                audio_node = audio_indices[0]\n",
    "                video_node = torch.where(video_mask)[0][0]\n",
    "                audio_edge_index = torch.tensor([[audio_node, video_node], [video_node, audio_node]], device=self.device)\n",
    "                try:\n",
    "                    print(f\"Debug: Audio GAT - nodes={x[audio_mask].size(0)}, edges={audio_edge_index.shape[1]}, max edge_index={audio_edge_index.max().item() if audio_edge_index.numel() > 0 else -1}\")\n",
    "                    h[audio_mask] = F.relu(self.gat_audio(x[audio_mask], audio_edge_index))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: GATConv audio failed: {e}\")\n",
    "    \n",
    "        try:\n",
    "            h = F.relu(self.gcn(h, ei))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: GCNConv failed: {e}\")\n",
    "            return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "    \n",
    "        h = self.proj(h)\n",
    "    \n",
    "        num_graphs = int(batch.max().item() + 1) if batch.numel() > 0 else 1\n",
    "        nodes_per_graph = int(np.ceil(h.size(0) / num_graphs))\n",
    "        pad_nodes = nodes_per_graph * num_graphs - h.size(0)\n",
    "        if pad_nodes:\n",
    "            h = torch.cat([h, torch.zeros(pad_nodes, h.size(1), device=self.device)])\n",
    "            node_types = torch.cat([node_types, torch.zeros(pad_nodes, dtype=torch.long, device=self.device)])\n",
    "            batch = torch.cat([batch, torch.full((pad_nodes,), -1, dtype=torch.long, device=self.device)])\n",
    "        h = h.view(num_graphs, nodes_per_graph, -1)\n",
    "    \n",
    "        try:\n",
    "            h, _ = self.attn(h, h, h)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: MultiheadAttention failed: {e}\")\n",
    "            return torch.zeros((num_graphs, 2), device=self.device)\n",
    "    \n",
    "        h = h.reshape(-1, h.size(-1))\n",
    "    \n",
    "        valid_mask = batch >= 0\n",
    "        h_valid = h[valid_mask]\n",
    "        batch_valid = batch[valid_mask]\n",
    "    \n",
    "        g_repr = torch.cat([\n",
    "            global_mean_pool(h_valid, batch_valid),\n",
    "            global_max_pool(h_valid, batch_valid)\n",
    "        ], dim=1)\n",
    "    \n",
    "        return self.classifier(g_repr)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 1.0, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.gamma, self.alpha = gamma, alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce).mean()\n",
    "\n",
    "def extract_label_from_entry(entry, video_path):\n",
    "    \"\"\"Extract label from metadata entry with multiple strategies.\"\"\"\n",
    "    if 'n_fakes' in entry:\n",
    "        n_fakes = entry['n_fakes']\n",
    "        if isinstance(n_fakes, int):\n",
    "            return 1 if n_fakes > 0 else 0\n",
    "    \n",
    "    if 'fake_periods' in entry:\n",
    "        fake_periods = entry['fake_periods']\n",
    "        if isinstance(fake_periods, list):\n",
    "            return 1 if len(fake_periods) > 0 else 0\n",
    "    \n",
    "    if 'label' in entry:\n",
    "        label = entry['label']\n",
    "        if isinstance(label, str):\n",
    "            return 1 if label.lower() in ['fake', 'deepfake', '1'] else 0\n",
    "        elif isinstance(label, int):\n",
    "            return label\n",
    "    \n",
    "    if 'is_fake' in entry:\n",
    "        return 1 if entry['is_fake'] else 0\n",
    "    \n",
    "    if 'original' in entry:\n",
    "        return 0 if entry['original'] else 1\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è No valid label field found for {video_path}\")\n",
    "    return None\n",
    "\n",
    "def create_filename_mapping(metadata):\n",
    "    \"\"\"Create a mapping of all possible filename variations to metadata entries.\"\"\"\n",
    "    filename_map = {}\n",
    "    \n",
    "    for entry in metadata:\n",
    "        if not isinstance(entry, dict) or 'file' not in entry:\n",
    "            continue\n",
    "            \n",
    "        metadata_file = entry['file']\n",
    "        variations = [\n",
    "            metadata_file,\n",
    "            os.path.basename(metadata_file),\n",
    "            os.path.splitext(os.path.basename(metadata_file))[0],\n",
    "            metadata_file.replace('\\\\', '/'),\n",
    "            metadata_file.replace('/', '\\\\'),\n",
    "        ]\n",
    "        \n",
    "        for variation in variations:\n",
    "            if variation not in filename_map:\n",
    "                filename_map[variation] = []\n",
    "            filename_map[variation].append(entry)\n",
    "    \n",
    "    return filename_map\n",
    "\n",
    "def find_label_with_mapping(video_path, filename_map, dataset_path):\n",
    "    \"\"\"Find label using the filename mapping with multiple strategies.\"\"\"\n",
    "    video_rel_path = os.path.relpath(video_path, dataset_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    video_name_no_ext = os.path.splitext(video_name)[0]\n",
    "    \n",
    "    search_keys = [\n",
    "        video_rel_path,\n",
    "        video_rel_path.replace('\\\\', '/'),\n",
    "        video_rel_path.replace('/', '\\\\'),\n",
    "        video_name,\n",
    "        video_name_no_ext,\n",
    "    ]\n",
    "    \n",
    "    for key in search_keys:\n",
    "        if key in filename_map:\n",
    "            entry = filename_map[key][0]\n",
    "            label = extract_label_from_entry(entry, video_path)\n",
    "            if label is not None:\n",
    "                return label\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_lavdf_dataset_improved(dataset_path, use_subset='train', max_clips=1000):\n",
    "    \"\"\"Improved dataset loading with better debugging and matching.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(\"‚ùå metadata.json not found!\")\n",
    "        return [], []\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    filename_map = create_filename_mapping(metadata)\n",
    "    \n",
    "    subset_folder = os.path.join(dataset_path, use_subset)\n",
    "    if not os.path.exists(subset_folder):\n",
    "        print(f\"‚ùå Subset folder '{use_subset}' not found\")\n",
    "        return [], []\n",
    "    \n",
    "    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "    all_videos = []\n",
    "    for ext in video_extensions:\n",
    "        all_videos.extend(glob.glob(os.path.join(subset_folder, '**', ext), recursive=True))\n",
    "    \n",
    "    if len(all_videos) > max_clips:\n",
    "        random.shuffle(all_videos)\n",
    "        all_videos = all_videos[:max_clips]\n",
    "    \n",
    "    video_audio_pairs = []\n",
    "    labels = []\n",
    "    matched_count = 0\n",
    "    unmatched_files = []\n",
    "    \n",
    "    for video_path in all_videos:\n",
    "        label = find_label_with_mapping(video_path, filename_map, dataset_path)\n",
    "        \n",
    "        if label is not None:\n",
    "            video_audio_pairs.append((video_path, video_path))\n",
    "            labels.append(label)\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            unmatched_files.append(video_path)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully matched: {matched_count}/{len(all_videos)} files\")\n",
    "    if labels:\n",
    "        real_count = labels.count(0)\n",
    "        fake_count = labels.count(1)\n",
    "        total = real_count + fake_count\n",
    "        print(f\"üìä Label distribution: Real={real_count} ({real_count/total*100:.1f}%), Fake={fake_count} ({fake_count/total*100:.1f}%)\")\n",
    "    \n",
    "    return video_audio_pairs, labels\n",
    "\n",
    "def load_all_subsets(dataset_path, max_clips=30):\n",
    "    \"\"\"Load and balance dataset across subsets.\"\"\"\n",
    "    all_pairs = []\n",
    "    all_labels = []\n",
    "    clips_per_subset = max_clips // 3\n",
    "    \n",
    "    for subset in ['train', 'test', 'dev']:\n",
    "        subset_path = os.path.join(dataset_path, subset)\n",
    "        if os.path.exists(subset_path):\n",
    "            pairs, labels = load_lavdf_dataset_improved(dataset_path, use_subset=subset, max_clips=clips_per_subset)\n",
    "            if pairs and labels:\n",
    "                all_pairs.extend(pairs)\n",
    "                all_labels.extend(labels)\n",
    "    \n",
    "    real_pairs = [p for p, l in zip(all_pairs, all_labels) if l == 0]\n",
    "    real_labels = [0] * len(real_pairs)\n",
    "    fake_pairs = [p for p, l in zip(all_pairs, all_labels) if l == 1]\n",
    "    fake_labels = [1] * len(fake_pairs)\n",
    "    \n",
    "    if len(real_pairs) < len(fake_pairs):\n",
    "        oversample_indices = np.random.choice(len(real_pairs), size=len(fake_pairs) - len(real_pairs), replace=True)\n",
    "        all_pairs.extend([real_pairs[i] for i in oversample_indices])\n",
    "        all_labels.extend([0] * len(oversample_indices))\n",
    "    elif len(fake_pairs) < len(real_pairs):\n",
    "        oversample_indices = np.random.choice(len(fake_pairs), size=len(real_pairs) - len(fake_pairs), replace=True)\n",
    "        all_pairs.extend([fake_pairs[i] for i in oversample_indices])\n",
    "        all_labels.extend([1] * len(oversample_indices))\n",
    "    \n",
    "    if len(all_pairs) > max_clips:\n",
    "        indices = np.random.choice(len(all_pairs), size=max_clips, replace=False)\n",
    "        all_pairs = [all_pairs[i] for i in indices]\n",
    "        all_labels = [all_labels[i] for i in indices]\n",
    "    \n",
    "    print(f\"Debug: Loaded {len(all_pairs)} samples, Real: {all_labels.count(0)}, Fake: {all_labels.count(1)}\")\n",
    "    return all_pairs, all_labels\n",
    "\n",
    "class DeepfakeDetector:\n",
    "    \"\"\"Main detector wrapper with extra runtime validation.\"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        self.feature_extractor = AudioVisualFeatureExtractor(device)\n",
    "        self.graph_constructor = GraphConstructor(similarity_threshold=0.6)\n",
    "        self.model = MultiModalGNN(device=device).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "        self.criterion = FocalLoss(gamma=1.0, alpha=0.5)\n",
    "\n",
    "    def prepare_data(self, video_audio_pairs, labels, feature_dir=None):\n",
    "        graphs = []\n",
    "        for i, ((v_path, a_path), label) in enumerate(zip(video_audio_pairs, labels)):\n",
    "            video_features = self.feature_extractor.extract_video_features(v_path)\n",
    "            audio_features = self.feature_extractor.extract_audio_features(a_path)\n",
    "            print(f\"Debug: Video features shape={video_features.shape}, mean={np.mean(video_features):.4f}, std={np.std(video_features):.4f}\")\n",
    "            print(f\"Debug: Audio features shape={audio_features.shape}, mean={np.mean(audio_features):.4f}, std={np.std(audio_features):.4f}\")\n",
    "            g = self.graph_constructor.create_graph(video_features, audio_features)\n",
    "            g = validate_edge_index(g)\n",
    "            g.y = torch.tensor([label], dtype=torch.long)\n",
    "            if g.edge_index.numel() > 0 and g.edge_index.max().item() >= g.x.size(0):\n",
    "                print(f\"Error: Graph {i} has invalid edge_index: max {g.edge_index.max().item()}, nodes {g.x.size(0)}\")\n",
    "                continue\n",
    "            graphs.append(g)\n",
    "        print(f\"Debug: Prepared {len(graphs)} valid graphs\")\n",
    "        return graphs\n",
    "        \n",
    "    def _clean_batch(self, batch):\n",
    "        \"\"\"Sanity-check a PyG Batch just before the forward pass.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            max_node = batch.x.size(0)\n",
    "            print(f\"Debug: Cleaning batch with {max_node} nodes, edge_index max = {batch.edge_index.max().item() if batch.edge_index.numel() > 0 else -1}\")\n",
    "            mask = (batch.edge_index[0] < max_node) & (batch.edge_index[1] < max_node)\n",
    "            if not mask.all():\n",
    "                print(f\"Debug: Filtering out {(~mask).sum()} invalid edges with indices >= {max_node}\")\n",
    "                batch.edge_index = batch.edge_index[:, mask]\n",
    "                if batch.edge_attr is not None and batch.edge_attr.size(0) == mask.size(0):\n",
    "                    batch.edge_attr = batch.edge_attr[mask]\n",
    "            print(f\"Debug: Batch cleaned, edge count = {batch.edge_index.shape[1]}, max index = {batch.edge_index.max().item() if batch.edge_index.numel() > 0 else -1}\")\n",
    "        return batch\n",
    "    \n",
    "    def train(self, train_graphs, val_graphs, epochs: int = 50, batch_size: int = 1, accum_steps: int = 2):\n",
    "        print(\"Debug: Initializing DataLoader with custom_collate\")\n",
    "        if not callable(custom_collate):\n",
    "            raise ValueError(\"custom_collate is not callable\")\n",
    "        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "        \n",
    "        scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "            for i, batch in enumerate(train_loader):\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    batch = self._clean_batch(batch)\n",
    "                    batch.y = batch.y.clamp(0, 1)\n",
    "        \n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batch)\n",
    "                    if outputs.size(0) != batch.y.size(0):\n",
    "                        print(f\"Warning: Batch/label size mismatch: outputs={outputs.size(0)}, labels={batch.y.size(0)}\")\n",
    "                        continue\n",
    "        \n",
    "                    loss = self.criterion(outputs, batch.y) / accum_steps\n",
    "                    loss.backward()\n",
    "        \n",
    "                    if (i + 1) % accum_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad()\n",
    "        \n",
    "                    train_loss += loss.item() * accum_steps\n",
    "                    train_total += batch.y.size(0)\n",
    "                    train_correct += (outputs.argmax(dim=1) == batch.y).sum().item()\n",
    "        \n",
    "                except Exception as exc:\n",
    "                    print(f\"‚ö†Ô∏è Train-batch {i} failed: {exc}\")\n",
    "                    continue\n",
    "        \n",
    "            train_acc = 100 * train_correct / max(1, train_total)\n",
    "        \n",
    "            self.model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    try:\n",
    "                        batch = batch.to(self.device)\n",
    "                        batch = self._clean_batch(batch)\n",
    "                        outputs = self.model(batch)\n",
    "                        loss = self.criterion(outputs, batch.y)\n",
    "        \n",
    "                        val_loss += loss.item()\n",
    "                        val_total += batch.y.size(0)\n",
    "                        val_correct += (outputs.argmax(dim=1) == batch.y).sum().item()\n",
    "        \n",
    "                    except Exception as exc:\n",
    "                        print(f\"‚ö†Ô∏è Val-batch failed: {exc}\")\n",
    "                        continue\n",
    "        \n",
    "            val_acc = 100 * val_correct / max(1, val_total)\n",
    "            scheduler.step()\n",
    "        \n",
    "            print(f\"Epoch {epoch+1:02}/{epochs} | Train Acc {train_acc:5.1f}% | Val Acc {val_acc:5.1f}%\")\n",
    "        \n",
    "            if val_acc > best_val_acc and val_total > 0:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\", _use_new_zipfile_serialization=True)\n",
    "                print(f\"üíæ New best model saved ({val_acc:.2f}%)\")\n",
    "        \n",
    "            if val_acc >= 80.0:\n",
    "                print(\"üéâ Target accuracy reached ‚Äì stopping early!\")\n",
    "                break\n",
    "        \n",
    "        return best_val_acc\n",
    "\n",
    "    def evaluate(self, test_graphs):\n",
    "        print(\"Debug: Initializing DataLoader with custom_collate for evaluation\")\n",
    "        if not callable(custom_collate):\n",
    "            raise ValueError(\"custom_collate is not callable\")\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n",
    "        except (FileNotFoundError, RuntimeError) as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load model: {e}. Re-training may be required.\")\n",
    "            return 0.0, 0.0, 0.0, 0.0\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(test_graphs, batch_size=1, shuffle=False, collate_fn=custom_collate)\n",
    "    \n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    batch = self._clean_batch(batch)\n",
    "                    batch.y = batch.y.clamp(0, 1)\n",
    "    \n",
    "                    outputs = self.model(batch)\n",
    "                    all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                    all_labels.extend(batch.y.cpu().numpy())\n",
    "                except Exception as exc:\n",
    "                    print(f\"‚ö†Ô∏è Test-batch {i} failed: {exc}\")\n",
    "                    continue\n",
    "    \n",
    "        acc = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\") if all_labels else (0.0, 0.0, 0.0, None)\n",
    "        print(f\"üìä Test Acc {acc:.4f} | Prec {prec:.4f} | Rec {rec:.4f} | F1 {f1:.4f}\")\n",
    "        return acc, prec, rec, f1\n",
    "    \n",
    "def debug_metadata_structure(dataset_path):\n",
    "    \"\"\"Debug the metadata structure.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(\"‚ùå metadata.json not found!\")\n",
    "        return None\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    if metadata:\n",
    "        sample_keys = set()\n",
    "        for entry in metadata[:10]:\n",
    "            if isinstance(entry, dict):\n",
    "                sample_keys.update(entry.keys())\n",
    "        print(f\"üìä Metadata entries: {len(metadata)}, Keys: {sample_keys}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ DEEPFAKE DETECTOR - CPU VERSION\")\n",
    "    \n",
    "    device = 'cpu'\n",
    "    torch.cuda.set_device(-1)\n",
    "    print(f\"üñ•Ô∏è Using device: CPU\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Warning: CUDA is available but forcing CPU usage\")\n",
    "        torch.cuda.set_device(-1)\n",
    "\n",
    "    if os.path.exists(\"best_model.pth\"):\n",
    "        os.remove(\"best_model.pth\")\n",
    "        print(\"Debug: Deleted existing best_model.pth\")\n",
    "\n",
    "    dataset_path = r\"C:\\archive\\LAV-DF\"\n",
    "    feature_dir = r\"C:\\Users\\ARNAV\\features\"  \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Dataset path does not exist: {dataset_path}\")\n",
    "        return\n",
    "    if not os.path.exists(feature_dir):\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nüìÇ LOADING DATASET...\")\n",
    "    video_audio_pairs, labels = load_all_subsets(dataset_path, max_clips=30)\n",
    "    \n",
    "    if len(video_audio_pairs) == 0:\n",
    "        print(\"‚ùå No valid data loaded. Please check dataset path and metadata.\")\n",
    "        debug_metadata_structure(dataset_path)\n",
    "        return\n",
    "        \n",
    "    detector = DeepfakeDetector(device=device)\n",
    "    detector.feature_extractor.fit_scalers(video_audio_pairs)\n",
    "    real_count = labels.count(0)\n",
    "    fake_count = labels.count(1)\n",
    "    print(f\"\\nüìä FINAL DATASET STATISTICS: Total samples: {len(video_audio_pairs)}, Real: {real_count} ({real_count/(real_count+fake_count)*100:.1f}%), Fake: {fake_count} ({fake_count/(real_count+fake_count)*100:.1f}%)\")\n",
    "    \n",
    "    if real_count == 0 or fake_count == 0:\n",
    "        print(\"‚ùå Dataset is severely imbalanced. Check metadata parsing.\")\n",
    "        debug_metadata_structure(dataset_path)\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è PRECOMPUTING FEATURES...\")\n",
    "    detector.feature_extractor.save_features(video_audio_pairs, feature_dir)\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è CREATING GRAPHS...\")\n",
    "    graphs = detector.prepare_data(video_audio_pairs, labels, feature_dir=feature_dir)\n",
    "    if len(graphs) < 10:\n",
    "        print(f\"‚ö†Ô∏è Only {len(graphs)} graphs created. Check feature extraction and graph construction.\")\n",
    "        return\n",
    "    \n",
    "    for i, g in enumerate(graphs):\n",
    "        if g.edge_index.numel() > 0 and g.edge_index.max().item() >= g.x.size(0):\n",
    "            print(f\"Error: Graph {i} has invalid edge_index: max {g.edge_index.max().item()}, nodes {g.x.size(0)}\")\n",
    "    \n",
    "    train_graphs, temp_graphs = train_test_split(graphs, test_size=0.3, random_state=42, stratify=labels)\n",
    "    val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42, stratify=[labels[i] for i in range(len(labels)) if graphs[i] in temp_graphs])\n",
    "    print(f\"Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}\")\n",
    "    \n",
    "    best_acc = detector.train(train_graphs, val_graphs, epochs=50, batch_size=1, accum_steps=2)\n",
    "    print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
    "    \n",
    "    if test_graphs:\n",
    "        accuracy, precision, recall, f1 = detector.evaluate(test_graphs)\n",
    "        print(f\"Final Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
