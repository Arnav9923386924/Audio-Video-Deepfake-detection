{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6334c4b-dd10-40ad-9ecb-ba18b9edfd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import librosa\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "\n",
    "# Enable CUDA if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "    os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return Batch.from_data_list(batch)\n",
    "    \n",
    "def validate_edge_index(data: Data) -> Data:\n",
    "    if data.edge_index.numel() == 0:\n",
    "        return data\n",
    "    max_nodes = data.x.size(0)\n",
    "    mask = (data.edge_index[0] < max_nodes) & (data.edge_index[1] < max_nodes)\n",
    "    if not mask.all():\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None and data.edge_attr.size(0) == mask.size(0):\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    return data\n",
    "\n",
    "class AudioVisualFeatureExtractor:\n",
    "    def __init__(self, device='cpu', max_samples=750):\n",
    "        self.device = device\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.scaler_video = StandardScaler()\n",
    "        self.scaler_audio = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "        self.max_samples = max_samples\n",
    "        self.audio_feature_dim = 35\n",
    "        \n",
    "    def extract_video_features(self, video_path, max_frames=30):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return np.zeros((max_frames, 1024))\n",
    "        facial_features = []\n",
    "        flow_features = []\n",
    "        prev_frame = None\n",
    "        frame_count = 0\n",
    "        while frame_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(frame_gray, 1.1, 4)\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = faces[0]\n",
    "                face_roi = frame_gray[y:y+h, x:x+w]\n",
    "                if face_roi.size > 0:\n",
    "                    face_resized = cv2.resize(face_roi, (32, 32))\n",
    "                    face_features = face_resized.flatten()\n",
    "                    if len(face_features) < 512:\n",
    "                        face_features = np.pad(face_features, (0, 512 - len(face_features)), 'constant')\n",
    "                    else:\n",
    "                        face_features = face_features[:512]\n",
    "                    facial_features.append(face_features)\n",
    "                else:\n",
    "                    facial_features.append(np.zeros(512))\n",
    "            else:\n",
    "                facial_features.append(np.zeros(512))\n",
    "            if prev_frame is not None:\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_frame, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow_flat = flow.flatten()\n",
    "                if len(flow_flat) >= 512:\n",
    "                    flow_features.append(flow_flat[:512])\n",
    "                else:\n",
    "                    padded_flow = np.pad(flow_flat, (0, 512 - len(flow_flat)), 'constant')\n",
    "                    flow_features.append(padded_flow)\n",
    "            else:\n",
    "                flow_features.append(np.zeros(512))\n",
    "            prev_frame = frame_gray\n",
    "            frame_count += 1\n",
    "        cap.release()\n",
    "        if len(facial_features) == 0:\n",
    "            facial_features = [np.zeros(512)]\n",
    "        if len(flow_features) == 0:\n",
    "            flow_features = [np.zeros(512)]\n",
    "        facial_features = np.array(facial_features)\n",
    "        flow_features = np.array(flow_features)\n",
    "        if len(facial_features) < max_frames:\n",
    "            pad_shape = ((0, max_frames - len(facial_features)), (0, 0))\n",
    "            facial_features = np.pad(facial_features, pad_shape, 'constant')\n",
    "            flow_features = np.pad(flow_features, pad_shape, 'constant')\n",
    "        else:\n",
    "            facial_features = facial_features[:max_frames]\n",
    "            flow_features = flow_features[:max_frames]\n",
    "        video_features = np.concatenate([facial_features, flow_features], axis=1)\n",
    "        if self.is_fitted:\n",
    "            video_features = self.scaler_video.transform(video_features)\n",
    "        return video_features\n",
    "    \n",
    "    def extract_audio_features(self, audio_path, max_length=5):\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                return np.zeros(self.audio_feature_dim)\n",
    "            import subprocess\n",
    "            ffprobe_cmd = f'ffprobe -i \"{audio_path}\" -show_streams -select_streams a -loglevel error'\n",
    "            result = subprocess.run(ffprobe_cmd, capture_output=True, text=True, shell=True)\n",
    "            if not result.stdout:\n",
    "                return np.zeros(self.audio_feature_dim)\n",
    "            if audio_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                temp_audio = os.path.join(os.environ.get('TEMP', os.getcwd()), \"temp_audio.wav\")\n",
    "                if os.name == 'nt':\n",
    "                    null_device = 'NUL'\n",
    "                else:\n",
    "                    null_device = '/dev/null'\n",
    "                cmd = f'ffmpeg -i \"{audio_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{temp_audio}\" -y'\n",
    "                result = os.system(f'{cmd} > {null_device} 2>&1')\n",
    "                if result != 0:\n",
    "                    return np.zeros(self.audio_feature_dim)\n",
    "                if not os.path.exists(temp_audio):\n",
    "                    return np.zeros(self.audio_feature_dim)\n",
    "                y, sr = librosa.load(temp_audio, sr=16000, duration=max_length)\n",
    "                os.remove(temp_audio)\n",
    "            else:\n",
    "                y, sr = librosa.load(audio_path, sr=16000, duration=max_length)\n",
    "            if len(y) == 0 or np.all(y == 0):\n",
    "                return np.zeros(self.audio_feature_dim)\n",
    "            if len(y) < sr * 0.5:\n",
    "                y = np.pad(y, (0, int(sr * 0.5) - len(y)), 'constant')\n",
    "            if random.random() < 0.3:\n",
    "                noise = np.random.normal(0, 0.005, len(y))\n",
    "                y = y + noise\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            zcr = librosa.feature.zero_crossing_rate(y)\n",
    "            if mfccs.shape[1] == 0 or spectral_centroids.shape[1] == 0:\n",
    "                return np.zeros(self.audio_feature_dim)\n",
    "            features = np.hstack([\n",
    "                np.mean(mfccs, axis=1),\n",
    "                np.mean(spectral_centroids, axis=1),\n",
    "                np.mean(spectral_rolloff, axis=1),\n",
    "                np.mean(chroma, axis=1),\n",
    "                np.mean(zcr, axis=1),\n",
    "            ])\n",
    "            expected_size = self.audio_feature_dim\n",
    "            if features.shape[0] != expected_size:\n",
    "                features = np.pad(features, (0, expected_size - features.shape[0]), 'constant') if features.shape[0] < expected_size else features[:expected_size]\n",
    "            if self.is_fitted:\n",
    "                features = self.scaler_audio.transform(features.reshape(1, -1)).flatten()\n",
    "            return features\n",
    "        except Exception:\n",
    "            return np.zeros(self.audio_feature_dim)\n",
    "    \n",
    "    def fit_scalers(self, video_audio_pairs):\n",
    "        all_video_features = []\n",
    "        all_audio_features = []\n",
    "        for i, (video_path, audio_path) in enumerate(video_audio_pairs[:self.max_samples]):\n",
    "            try:\n",
    "                video_feat = self.extract_video_features(video_path)\n",
    "                audio_feat = self.extract_audio_features(audio_path)\n",
    "                all_video_features.append(video_feat)\n",
    "                all_audio_features.append(audio_feat)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if all_video_features and all_audio_features:\n",
    "            all_video_features = np.array(all_video_features)\n",
    "            all_audio_features = np.array(all_audio_features)\n",
    "            if all_video_features.shape[0] != self.max_samples or all_audio_features.shape[0] != self.max_samples:\n",
    "                min_samples = min(all_video_features.shape[0], all_audio_features.shape[0], self.max_samples)\n",
    "                all_video_features = all_video_features[:min_samples]\n",
    "                all_audio_features = all_audio_features[:min_samples]\n",
    "            all_video_features = all_video_features.reshape(-1, all_video_features.shape[-1])\n",
    "            all_audio_features = all_audio_features.reshape(-1, all_audio_features.shape[-1])\n",
    "            self.scaler_video.fit(all_video_features)\n",
    "            self.scaler_audio.fit(all_audio_features)\n",
    "            self.is_fitted = True\n",
    "    \n",
    "    def save_features(self, video_audio_pairs, feature_dir):\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "        saved_count = 0\n",
    "        for i, (video_path, audio_path) in enumerate(video_audio_pairs[:self.max_samples]):\n",
    "            try:\n",
    "                video_features = self.extract_video_features(video_path)\n",
    "                audio_features = self.extract_audio_features(audio_path)\n",
    "                if video_features.shape != (30, 1024):\n",
    "                    video_features = np.zeros((30, 1024))\n",
    "                if audio_features.shape != (self.audio_feature_dim,):\n",
    "                    audio_features = np.zeros(self.audio_feature_dim)\n",
    "                video_file = os.path.join(feature_dir, f\"video_{i}.npy\")\n",
    "                audio_file = os.path.join(feature_dir, f\"audio_{i}.npy\")\n",
    "                np.save(video_file, video_features)\n",
    "                np.save(audio_file, audio_features)\n",
    "                saved_count += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "class GraphConstructor:\n",
    "    def __init__(self, similarity_threshold=0.7, device='cpu'):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.device = device\n",
    "    \n",
    "    def _finalise_edges(self, edge_index_list, edge_attr_list, n_nodes):\n",
    "        if not edge_index_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long), torch.zeros(0)\n",
    "        ei = np.asarray(edge_index_list, dtype=np.int64)\n",
    "        ea = np.asarray(edge_attr_list, dtype=np.float32)\n",
    "        valid = (ei[:, 0] < n_nodes) & (ei[:, 1] < n_nodes)\n",
    "        ei = ei[valid]\n",
    "        ea = ea[valid] if ea.shape[0] == valid.size else ea[:valid.sum()]\n",
    "        if ei.size == 0:\n",
    "            return torch.zeros((2, 0), dtype=torch.long), torch.zeros(0)\n",
    "        return torch.from_numpy(ei).t().contiguous(), torch.from_numpy(ea)\n",
    "        \n",
    "    def create_graph(self, video_features: np.ndarray, audio_features: np.ndarray) -> Data:\n",
    "        all_features, node_types = [], []\n",
    "        for frame in video_features:\n",
    "            all_features.append(frame)\n",
    "            node_types.append(0)\n",
    "        audio_added = False\n",
    "        if audio_features.size > 0 and np.any(audio_features) and audio_features.shape[0] == 35:\n",
    "            audio_expanded = np.tile(audio_features, (1024 // audio_features.size + 1))[:1024]\n",
    "            all_features.append(audio_expanded)\n",
    "            node_types.append(1)\n",
    "            audio_added = True\n",
    "        else:\n",
    "            all_features.append(np.zeros(1024))\n",
    "            node_types.append(1)\n",
    "            audio_added = True\n",
    "        if len(all_features) < 2:\n",
    "            all_features.extend([np.zeros(1024), np.zeros(1024)])\n",
    "            node_types.extend([0, 1])\n",
    "        all_features = np.asarray(all_features, dtype=np.float32)\n",
    "        n_nodes = all_features.shape[0]\n",
    "        edge_index_list, edge_attr_list = [], []\n",
    "        audio_node_idx = n_nodes - 1 if audio_added else None\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i + 1, n_nodes):\n",
    "                if node_types[i] != node_types[j]:\n",
    "                    edge_index_list.extend([[i, j], [j, i]])\n",
    "                    edge_attr_list.extend([0.5, 0.5])\n",
    "                else:\n",
    "                    sim = self._cosine(all_features[i], all_features[j])\n",
    "                    if sim > self.similarity_threshold:\n",
    "                        edge_index_list.extend([[i, j], [j, i]])\n",
    "                        edge_attr_list.extend([sim, sim])\n",
    "        if audio_added and audio_node_idx is not None:\n",
    "            audio_edges = [[i, audio_node_idx] for i in range(n_nodes-1) if node_types[i] == 0]\n",
    "            audio_edges.extend([[audio_node_idx, i] for i in range(n_nodes-1) if node_types[i] == 0])\n",
    "            edge_index_list.extend(audio_edges)\n",
    "            edge_attr_list.extend([0.5] * len(audio_edges))\n",
    "        edge_index, edge_attr = self._finalise_edges(edge_index_list, edge_attr_list, n_nodes)\n",
    "        if not edge_index_list:\n",
    "            for j in range(1, min(n_nodes, 2)):\n",
    "                edge_index_list.extend([[0, j], [j, 0]])\n",
    "                edge_attr_list.extend([0.0, 0.0])\n",
    "            edge_index, edge_attr = self._finalise_edges(edge_index_list, edge_attr_list, n_nodes)\n",
    "        data = Data(\n",
    "            x=torch.from_numpy(all_features).to(torch.device(self.device)),\n",
    "            edge_index=edge_index.to(torch.device(self.device)),\n",
    "            edge_attr=edge_attr.to(torch.device(self.device)),\n",
    "            node_types=torch.tensor(node_types, dtype=torch.long).to(torch.device(self.device)),\n",
    "            batch=torch.zeros(n_nodes, dtype=torch.long).to(torch.device(self.device)),\n",
    "        )\n",
    "        data = validate_edge_index(data)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine(a, b):\n",
    "        na, nb = np.linalg.norm(a), np.linalg.norm(b)\n",
    "        return 0.0 if na == 0 or nb == 0 else float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "class MultiModalGNN(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, hidden_dim: int = 512, num_classes: int = 2, dropout: float = 0.3, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.node_type_embedding = nn.Embedding(2, 128).to(self.device)\n",
    "        self.video_proj = nn.Linear(input_dim, hidden_dim).to(self.device)\n",
    "        self.audio_proj = nn.Linear(input_dim, hidden_dim).to(self.device)\n",
    "        self.gat_video = GATConv(hidden_dim + 128, hidden_dim, heads=8, dropout=dropout, concat=False).to(self.device)\n",
    "        self.gat_audio = GATConv(hidden_dim + 128, hidden_dim, heads=8, dropout=dropout, concat=False).to(self.device)\n",
    "        self.gcn = GCNConv(hidden_dim, hidden_dim // 2).to(self.device)\n",
    "        self.proj = nn.Linear(hidden_dim // 2, hidden_dim // 2).to(self.device)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim // 2, num_heads=8, dropout=dropout, batch_first=True).to(self.device)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        ).to(self.device)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        data = data.to(self.device)\n",
    "        x, ei, batch, node_types = data.x.to(self.device), data.edge_index.to(self.device), data.batch.to(self.device), data.node_types.to(self.device)\n",
    "        try:\n",
    "            if node_types.min() < 0 or node_types.max() > 1:\n",
    "                return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "            if hasattr(data, 'y') and (data.y.min() < 0 or data.y.max() > 1):\n",
    "                return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "            if ei.numel() > 0 and ei.max() >= x.size(0):\n",
    "                mask = (ei[0] < x.size(0)) & (ei[1] < x.size(0))\n",
    "                ei = ei[:, mask]\n",
    "                if data.edge_attr is not None and data.edge_attr.size(0) == mask.size(0):\n",
    "                    data.edge_attr = data.edge_attr[mask]\n",
    "                data.edge_index = ei\n",
    "        except Exception:\n",
    "            return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "        type_emb = self.node_type_embedding(node_types)\n",
    "        video_mask = node_types == 0\n",
    "        audio_mask = node_types == 1\n",
    "        x_proj = torch.zeros(x.size(0), self.video_proj.out_features, device=self.device)\n",
    "        if video_mask.any():\n",
    "            x_proj[video_mask] = F.relu(self.video_proj(x[video_mask]))\n",
    "        if audio_mask.any():\n",
    "            x_proj[audio_mask] = F.relu(self.audio_proj(x[audio_mask]))\n",
    "        x = torch.cat([x_proj, type_emb], dim=1)\n",
    "        h = torch.zeros_like(x_proj, device=self.device)\n",
    "        if video_mask.any():\n",
    "            video_indices = torch.where(video_mask)[0]\n",
    "            video_mask_edge = torch.isin(ei[0], video_indices) & torch.isin(ei[1], video_indices)\n",
    "            video_edge_index = ei[:, video_mask_edge]\n",
    "            if video_edge_index.numel() > 0:\n",
    "                video_node_map = torch.zeros(x.size(0), dtype=torch.long, device=self.device)\n",
    "                video_node_map[video_indices] = torch.arange(video_indices.size(0), device=self.device)\n",
    "                video_edge_index = video_node_map[video_edge_index]\n",
    "                try:\n",
    "                    h[video_mask] = F.relu(self.gat_video(x[video_mask], video_edge_index))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if audio_mask.any():\n",
    "            audio_indices = torch.where(audio_mask)[0]\n",
    "            audio_mask_edge = torch.isin(ei[0], audio_indices) | torch.isin(ei[1], audio_indices)\n",
    "            audio_edge_index = ei[:, audio_mask_edge]\n",
    "            if audio_edge_index.numel() > 0:\n",
    "                audio_node_map = torch.zeros(x.size(0), dtype=torch.long, device=self.device)\n",
    "                audio_node_map[audio_indices] = torch.arange(audio_indices.size(0), device=self.device)\n",
    "                audio_edge_index = audio_node_map[audio_edge_index]\n",
    "                try:\n",
    "                    h[audio_mask] = F.relu(self.gat_audio(x[audio_mask], audio_edge_index))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                audio_node = audio_indices[0]\n",
    "                video_node = torch.where(video_mask)[0][0]\n",
    "                audio_edge_index = torch.tensor([[audio_node, video_node], [video_node, audio_node]], device=self.device)\n",
    "                try:\n",
    "                    h[audio_mask] = F.relu(self.gat_audio(x[audio_mask], audio_edge_index))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        try:\n",
    "            h = F.relu(self.gcn(h, ei))\n",
    "        except Exception:\n",
    "            return torch.zeros((batch.max().item() + 1, 2), device=self.device)\n",
    "        h = self.proj(h)\n",
    "        num_graphs = int(batch.max().item() + 1) if batch.numel() > 0 else 1\n",
    "        nodes_per_graph = int(np.ceil(h.size(0) / num_graphs))\n",
    "        pad_nodes = nodes_per_graph * num_graphs - h.size(0)\n",
    "        if pad_nodes:\n",
    "            h = torch.cat([h, torch.zeros(pad_nodes, h.size(1), device=self.device)])\n",
    "            node_types = torch.cat([node_types, torch.zeros(pad_nodes, dtype=torch.long, device=self.device)])\n",
    "            batch = torch.cat([batch, torch.full((pad_nodes,), -1, dtype=torch.long, device=self.device)])\n",
    "        h = h.view(num_graphs, nodes_per_graph, -1)\n",
    "        try:\n",
    "            h, _ = self.attn(h, h, h)\n",
    "        except Exception:\n",
    "            return torch.zeros((num_graphs, 2), device=self.device)\n",
    "        h = h.reshape(-1, h.size(-1))\n",
    "        valid_mask = batch >= 0\n",
    "        h_valid = h[valid_mask]\n",
    "        batch_valid = batch[valid_mask]\n",
    "        g_repr = torch.cat([\n",
    "            global_mean_pool(h_valid, batch_valid),\n",
    "            global_max_pool(h_valid, batch_valid)\n",
    "        ], dim=1)\n",
    "        return self.classifier(g_repr)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 1.5, alpha: float = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction=\"none\", weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        return ((1 - pt) ** self.gamma * ce).mean()\n",
    "\n",
    "def extract_label_from_entry(entry, video_path):\n",
    "    if 'n_fakes' in entry:\n",
    "        n_fakes = entry['n_fakes']\n",
    "        if isinstance(n_fakes, int):\n",
    "            return 1 if n_fakes > 0 else 0\n",
    "    if 'fake_periods' in entry:\n",
    "        fake_periods = entry['fake_periods']\n",
    "        if isinstance(fake_periods, list):\n",
    "            return 1 if len(fake_periods) > 0 else 0\n",
    "    if 'label' in entry:\n",
    "        label = entry['label']\n",
    "        if isinstance(label, str):\n",
    "            return 1 if label.lower() in ['fake', 'deepfake', '1'] else 0\n",
    "        elif isinstance(label, int):\n",
    "            return label\n",
    "    if 'is_fake' in entry:\n",
    "        return 1 if entry['is_fake'] else 0\n",
    "    if 'original' in entry:\n",
    "        return 0 if entry['original'] else 1\n",
    "    return None\n",
    "\n",
    "def create_filename_mapping(metadata):\n",
    "    filename_map = {}\n",
    "    for entry in metadata:\n",
    "        if not isinstance(entry, dict) or 'file' not in entry:\n",
    "            continue\n",
    "        metadata_file = entry['file']\n",
    "        variations = [\n",
    "            metadata_file,\n",
    "            os.path.basename(metadata_file),\n",
    "            os.path.splitext(os.path.basename(metadata_file))[0],\n",
    "            metadata_file.replace('\\\\', '/'),\n",
    "            metadata_file.replace('/', '\\\\'),\n",
    "        ]\n",
    "        for variation in variations:\n",
    "            if variation not in filename_map:\n",
    "                filename_map[variation] = []\n",
    "            filename_map[variation].append(entry)\n",
    "    return filename_map\n",
    "\n",
    "def find_label_with_mapping(video_path, filename_map, dataset_path):\n",
    "    video_rel_path = os.path.relpath(video_path, dataset_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    video_name_no_ext = os.path.splitext(video_name)[0]\n",
    "    search_keys = [\n",
    "        video_rel_path,\n",
    "        video_rel_path.replace('\\\\', '/'),\n",
    "        video_rel_path.replace('/', '\\\\'),\n",
    "        video_name,\n",
    "        video_name_no_ext,\n",
    "    ]\n",
    "    for key in search_keys:\n",
    "        if key in filename_map:\n",
    "            entry = filename_map[key][0]\n",
    "            label = extract_label_from_entry(entry, video_path)\n",
    "            if label is not None:\n",
    "                return label\n",
    "    return None\n",
    "\n",
    "def load_lavdf_dataset_improved(dataset_path, use_subset='train', max_clips=750):\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return [], []\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    filename_map = create_filename_mapping(metadata)\n",
    "    subset_folder = os.path.join(dataset_path, use_subset)\n",
    "    if not os.path.exists(subset_folder):\n",
    "        return [], []\n",
    "    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "    all_videos = []\n",
    "    for ext in video_extensions:\n",
    "        all_videos.extend(glob.glob(os.path.join(subset_folder, '**', ext), recursive=True))\n",
    "    if len(all_videos) > max_clips:\n",
    "        random.shuffle(all_videos)\n",
    "        all_videos = all_videos[:max_clips]\n",
    "    video_audio_pairs = []\n",
    "    labels = []\n",
    "    matched_count = 0\n",
    "    for video_path in all_videos:\n",
    "        label = find_label_with_mapping(video_path, filename_map, dataset_path)\n",
    "        if label is not None:\n",
    "            video_audio_pairs.append((video_path, video_path))\n",
    "            labels.append(label)\n",
    "            matched_count += 1\n",
    "    return video_audio_pairs, labels\n",
    "\n",
    "def load_all_subsets(dataset_path, max_clips=750):\n",
    "    all_pairs = []\n",
    "    all_labels = []\n",
    "    clips_per_subset = max_clips // 3\n",
    "    for subset in ['train', 'test', 'dev']:\n",
    "        subset_path = os.path.join(dataset_path, subset)\n",
    "        if os.path.exists(subset_path):\n",
    "            pairs, labels = load_lavdf_dataset_improved(dataset_path, use_subset=subset, max_clips=clips_per_subset)\n",
    "            if pairs and labels:\n",
    "                all_pairs.extend(pairs)\n",
    "                all_labels.extend(labels)\n",
    "    real_pairs = [p for p, l in zip(all_pairs, all_labels) if l == 0]\n",
    "    real_labels = [0] * len(real_pairs)\n",
    "    fake_pairs = [p for p, l in zip(all_pairs, all_labels) if l == 1]\n",
    "    fake_labels = [1] * len(fake_pairs)\n",
    "    if len(real_pairs) < len(fake_pairs):\n",
    "        oversample_indices = np.random.choice(len(real_pairs), size=len(fake_pairs) - len(real_pairs), replace=True)\n",
    "        all_pairs.extend([real_pairs[i] for i in oversample_indices])\n",
    "        all_labels.extend([0] * len(oversample_indices))\n",
    "    elif len(fake_pairs) < len(real_pairs):\n",
    "        oversample_indices = np.random.choice(len(fake_pairs), size=len(real_pairs) - len(fake_pairs), replace=True)\n",
    "        all_pairs.extend([fake_pairs[i] for i in oversample_indices])\n",
    "        all_labels.extend([1] * len(oversample_indices))\n",
    "    if len(all_pairs) > max_clips:\n",
    "        indices = np.random.choice(len(all_pairs), size=max_clips, replace=False)\n",
    "        all_pairs = [all_pairs[i] for i in indices]\n",
    "        all_labels = [all_labels[i] for i in indices]\n",
    "    return all_pairs, all_labels\n",
    "\n",
    "class DeepfakeDetector:\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        self.feature_extractor = AudioVisualFeatureExtractor(device)\n",
    "        self.graph_constructor = GraphConstructor(similarity_threshold=0.7, device=device)\n",
    "        self.model = MultiModalGNN(device=device).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "        self.criterion = FocalLoss(gamma=1.5, alpha=None)\n",
    "\n",
    "    def prepare_data(self, video_audio_pairs, labels, feature_dir=None):\n",
    "        graphs = []\n",
    "        for i, ((v_path, a_path), label) in enumerate(zip(video_audio_pairs, labels)):\n",
    "            video_features = self.feature_extractor.extract_video_features(v_path)\n",
    "            audio_features = self.feature_extractor.extract_audio_features(a_path)\n",
    "            g = self.graph_constructor.create_graph(video_features, audio_features)\n",
    "            g = validate_edge_index(g)\n",
    "            g.y = torch.tensor([label], dtype=torch.long)\n",
    "            if g.edge_index.numel() > 0 and g.edge_index.max().item() >= g.x.size(0):\n",
    "                continue\n",
    "            graphs.append(g)\n",
    "        return graphs\n",
    "        \n",
    "    def _clean_batch(self, batch):\n",
    "        with torch.no_grad():\n",
    "            max_node = batch.x.size(0)\n",
    "            mask = (batch.edge_index[0] < max_node) & (batch.edge_index[1] < max_node)\n",
    "            if not mask.all():\n",
    "                batch.edge_index = batch.edge_index[:, mask]\n",
    "                if batch.edge_attr is not None and batch.edge_attr.size(0) == mask.size(0):\n",
    "                    batch.edge_attr = batch.edge_attr[mask]\n",
    "        return batch\n",
    "    \n",
    "    def train(self, train_graphs, val_graphs, epochs: int = 100, batch_size: int = 8, accum_steps: int = 4):\n",
    "        labels = [g.y.item() for g in train_graphs]\n",
    "        real_count = labels.count(0)\n",
    "        fake_count = labels.count(1)\n",
    "        total = real_count + fake_count\n",
    "        if real_count > 0 and fake_count > 0:\n",
    "            weight_real = total / (2 * real_count)\n",
    "            weight_fake = total / (2 * fake_count)\n",
    "            self.criterion.alpha = torch.tensor([weight_real, weight_fake], device=self.device)\n",
    "        else:\n",
    "            self.criterion.alpha = None\n",
    "        train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < 10:  # Warmup for first 10 epochs\n",
    "                return (epoch + 1) / 10\n",
    "            return 1.0\n",
    "        scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs-10)\n",
    "        warmup_scheduler = LambdaLR(self.optimizer, lr_lambda)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        metrics_file = os.path.join(\"C:\\\\Users\\\\ARNAV\", \"metrics.txt\")\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            f.write(\"Epoch,Train Accuracy,Validation Accuracy\\n\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    batch = self._clean_batch(batch)\n",
    "                    batch.y = batch.y.clamp(0, 1)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(batch)\n",
    "                    if outputs.size(0) != batch.y.size(0):\n",
    "                        continue\n",
    "                    loss = self.criterion(outputs, batch.y) / accum_steps\n",
    "                    loss.backward()\n",
    "                    if (i + 1) % accum_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad()\n",
    "                    train_loss += loss.item() * accum_steps\n",
    "                    train_total += batch.y.size(0)\n",
    "                    train_correct += (outputs.argmax(dim=1) == batch.y).sum().item()\n",
    "                except Exception:\n",
    "                    continue\n",
    "            train_acc = 100 * train_correct / max(1, train_total)\n",
    "            self.model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    try:\n",
    "                        batch = batch.to(self.device)\n",
    "                        batch = self._clean_batch(batch)\n",
    "                        outputs = self.model(batch)\n",
    "                        loss = self.criterion(outputs, batch.y)\n",
    "                        val_loss += loss.item()\n",
    "                        val_total += batch.y.size(0)\n",
    "                        val_correct += (outputs.argmax(dim=1) == batch.y).sum().item()\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            val_acc = 100 * val_correct / max(1, val_total)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "            with open(metrics_file, 'a') as f:\n",
    "                f.write(f\"{epoch+1},{train_acc:.2f},{val_acc:.2f}\\n\")\n",
    "            if epoch < 10:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if val_acc > best_val_acc and val_total > 0:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\", _use_new_zipfile_serialization=True)\n",
    "            if val_acc >= 80.0:\n",
    "                print(f\"Reached validation accuracy of {val_acc:.2f}%, stopping training.\")\n",
    "                break\n",
    "        return best_val_acc\n",
    "\n",
    "    def evaluate(self, test_graphs):\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            print(\"Error: Could not load model weights. Returning zero metrics.\")\n",
    "            return 0.0, 0.0, 0.0, 0.0\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(test_graphs, batch_size=8, shuffle=False, collate_fn=custom_collate)\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_loader):\n",
    "                try:\n",
    "                    batch = batch.to(self.device)\n",
    "                    batch = self._clean_batch(batch)\n",
    "                    batch.y = batch.y.clamp(0, 1)\n",
    "                    outputs = self.model(batch)\n",
    "                    all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                    all_labels.extend(batch.y.cpu().numpy())\n",
    "                except Exception:\n",
    "                    continue\n",
    "        acc = accuracy_score(all_labels, all_preds) if all_labels else 0.0\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\") if all_labels else (0.0, 0.0, 0.0, None)\n",
    "        print(f\"Test Metrics: Accuracy: {acc*100:.2f}%, Precision: {prec*100:.2f}%, Recall: {rec*100:.2f}%, F1-Score: {f1*100:.2f}%\")\n",
    "        metrics_file = os.path.join(\"C:\\\\Users\\\\ARNAV\", \"metrics.txt\")\n",
    "        with open(metrics_file, 'a') as f:\n",
    "            f.write(f\"Test Metrics: Accuracy: {acc*100:.2f}%, Precision: {prec*100:.2f}%, Recall: {rec*100:.2f}%, F1-Score: {f1*100:.2f}%\\n\")\n",
    "        return acc, prec, rec, f1\n",
    "    \n",
    "def debug_metadata_structure(dataset_path):\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        return None\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    return metadata\n",
    "\n",
    "def main():\n",
    "    dataset_path = r\"C:\\archive\\LAV-DF\"\n",
    "    feature_dir = r\"C:\\Users\\ARNAV\\features\"  \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Error: Dataset path {dataset_path} does not exist.\")\n",
    "        return\n",
    "    if not os.path.exists(feature_dir):\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "    video_audio_pairs, labels = load_all_subsets(dataset_path, max_clips=750)\n",
    "    if len(video_audio_pairs) == 0:\n",
    "        print(\"Error: No video-audio pairs loaded. Check metadata structure.\")\n",
    "        debug_metadata_structure(dataset_path)\n",
    "        return\n",
    "    detector = DeepfakeDetector(device=device)\n",
    "    detector.feature_extractor.fit_scalers(video_audio_pairs)\n",
    "    real_count = labels.count(0)\n",
    "    fake_count = labels.count(1)\n",
    "    if real_count == 0 or fake_count == 0:\n",
    "        print(\"Error: Dataset contains only one class. Check metadata structure.\")\n",
    "        debug_metadata_structure(dataset_path)\n",
    "        return\n",
    "    detector.feature_extractor.save_features(video_audio_pairs, feature_dir)\n",
    "    graphs = detector.prepare_data(video_audio_pairs, labels, feature_dir=feature_dir)\n",
    "    if len(graphs) < 10:\n",
    "        print(\"Error: Too few graphs generated. Check feature extraction.\")\n",
    "        return\n",
    "    train_graphs, temp_graphs = train_test_split(graphs, test_size=0.3, random_state=42, stratify=labels)\n",
    "    val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.3333, random_state=42, stratify=[labels[i] for i in range(len(labels)) if graphs[i] in temp_graphs])\n",
    "    best_acc = detector.train(train_graphs, val_graphs, epochs=100, batch_size=8, accum_steps=4)\n",
    "    print(f\"Best Validation Accuracy: {best_acc:.2f}%\")\n",
    "    if test_graphs:\n",
    "        accuracy, precision, recall, f1 = detector.evaluate(test_graphs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efb7c0-d9fe-44de-98a3-885711c96b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
